---
title: "Load Fishglob Database and Apply Temporal and Spatial Standardization"
output: html_notebook
---

This code is Script 1 for Kitchel et al. TITLE manuscript.

- This project is a collaborative effort to describe changes in taxonomic composition  of fish communities around the world--as sampled by bottom trawl surveys.

- Code by ZoÃ« J. Kitchel

SESSION INFO

R version 4.2.1 (2022-06-23)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Big Sur 11.7

Matrix products: default
LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] cowplot_1.1.1     sf_1.0-9          here_1.0.1        googledrive_2.0.0
 [5] geosphere_1.5-14  taxize_0.9.100    rgdal_1.6-2       data.table_1.14.4
 [9] rasterVis_0.51.4  lattice_0.20-45   gridExtra_2.3     viridis_0.6.2    
[13] viridisLite_0.4.1 rgbif_3.7.3       rgeos_0.5-9       raster_3.6-11    
[17] sp_1.5-0          tidyr_1.2.1       dplyr_1.0.10     

loaded via a namespace (and not attached):
 [1] nlme_3.1-157        fs_1.5.2            bold_1.2.0          oai_0.4.0          
 [5] RColorBrewer_1.1-3  httr_1.4.4          rprojroot_2.0.3     bslib_0.4.0        
 [9] tools_4.2.1         utf8_1.2.2          R6_2.5.1            KernSmooth_2.23-20 
[13] DBI_1.1.3           lazyeval_0.2.2      colorspace_2.0-3    tidyselect_1.2.0   
[17] curl_4.3.3          compiler_4.2.1      cli_3.4.1           xml2_1.3.3         
[21] sass_0.4.2          scales_1.2.1        classInt_0.4-8      hexbin_1.28.2      
[25] proxy_0.4-27        stringr_1.4.1       digest_0.6.30       rmarkdown_2.17     
[29] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.3     fastmap_1.1.0      
[33] rlang_1.0.6         rstudioapi_0.14     httpcode_0.3.0      jquerylib_0.1.4    
[37] generics_0.1.3      zoo_1.8-11          jsonlite_1.8.3      magrittr_2.0.3     
[41] interp_1.1-3        Rcpp_1.0.9          munsell_0.5.0       fansi_1.0.3        
[45] ape_5.6-2           lifecycle_1.0.3     terra_1.6-41        stringi_1.7.8      
[49] whisker_0.4         yaml_2.3.6          plyr_1.8.7          grid_4.2.1         
[53] parallel_4.2.1      crayon_1.5.2        deldir_1.0-6        conditionz_0.1.0   
[57] knitr_1.40          pillar_1.8.1        uuid_1.1-0          codetools_0.2-18   
[61] crul_1.3            glue_1.6.2          evaluate_0.17       latticeExtra_0.6-30
[65] png_0.1-7           vctrs_0.5.0         foreach_1.5.2       gtable_0.3.1       
[69] purrr_0.3.5         reshape_0.8.9       assertthat_0.2.1    cachem_1.0.6       
[73] ggplot2_3.3.6       xfun_0.34           e1071_1.7-12        class_7.3-20       
[77] gargle_1.2.1        tibble_3.1.8        iterators_1.0.14    units_0.8-0 

```{r setup}
library(tidyverse)
library(sp)
library(raster)
library(rgeos)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
library(concaveman)
library(sf)
library(viridis)
set.seed(1)
```


```{r}
#library(dggridR) #no longer works will find alternate method
library(data.table)
library(rgdal)
library(raster)
library(sp)
#library(rnaturalearth)
#library(rnaturalearthdata)
library(rgeos)
library(taxize) #standardizing names
library(geosphere)  #to calculate distance between lat lon of grid cells
library(googledrive)
library(here)
library(sf)
library(cowplot)
```


Pull in compiled and cleaned data from FishGlob, downloaded on November 28, 2022 (V 1.5) at [this link](https://drive.google.com/file/d/1MgXKhmIufUtjE_Y_mWpDeldJKfd7nDEM/view?usp=share_link). This is typically compiled by Aurore Maureaud.

```{r pull in fishglob database}

FishGlob_1.5 <- fread(here::here("data","FISHGLOB_v1.5_clean.csv"))

#add new column for season/quarter because survey_unit does not always show season correctly (i.e. GMEX)

#check first which survey_unit columns are okay and not
View(unique(FishGlob_1.5[,.(survey, season, quarter, survey_unit)]))

table(FishGlob_1.5$survey, FishGlob_1.5$quarter)

#not completely dealing with this now, but will atleast add in season for GMEX to survey unit for now

FishGlob_1.5[survey == "GMEX", survey_unit := paste0(survey,"-",season)]

```

Specific Regional Fixes

GSL
North: we have data 1980-2019, but gear changes in 2004/2005, so let's use later portion (more consistent months of sampling; 2005-2019; 15 years)
South: we have data 1970-2019, but gear/vessel changes in 1985 and again in 1992, so again let's use later portion (1992-2019; 27 years)

```{r GSL fixes}
#identify haul_ids of hauls we should remove
haul_ids_to_remove_GSL <- unique(FishGlob_1.5[(survey == "GSL-N" & year < 2005)|(survey == "GSL-S" & year < 1992),haul_id])

FishGlob_1.5 <- FishGlob_1.5[!(haul_id %in% haul_ids_to_remove_GSL),] #remove hauls before consistent gear/vessel was used
```

##Spatial and Temporal Patterns in All Trawl Surveys
Let's look at hauls per year/month and year/quarter and year/season visually
```{r}
#unique haul, survey, quarter, season, year

FishGlob.uniquehauls <- unique(FishGlob_1.5[,.(survey, survey_unit, year,month,quarter,season,haul_id, latitude, longitude)])

#add column with adjusted longitude for few regions that cross dateline (NZ and AI)
FishGlob.uniquehauls[,longitude_adj := ifelse((survey_unit %in% c("AI","NZ-CHAT") & longitude > 0),longitude-360,longitude)]

FishGlob.uniquehauls[,haul_counts_per_survey_season_month :=uniqueN(haul_id),.(survey, month, season)][, #count # hauls per survey, season, and month
                     haul_counts_per_survey_quarter_month :=uniqueN(haul_id),.(survey, month, quarter)][,#count # hauls per survey, month, and quarter
                     total_hauls_survey :=uniqueN(haul_id),.(survey)][,#count # hauls per survey in all years
                                                        
              #proportion of hauls for each survey, season, and month divided by total # over all years
                     haul_proportion_survey_season :=haul_counts_per_survey_season_month/total_hauls_survey][,
              #proportion of hauls for each survey, quarter, and month divided by total # over all years
                     haul_proportion_survey_quarter :=haul_counts_per_survey_quarter_month/total_hauls_survey][,
                                                                                                               
                     haul_count_per_survey_year_month :=uniqueN(haul_id),.(year, survey_unit, month)][, #count # hauls per survey unit, year, and month
                     total_hauls_survey_year := uniqueN(haul_id),.(survey_unit,year)][, #count total # hauls per survey unit and year
                     #proportion of hauls for each survey unit and month divided by total # hauls within a survey unit within a year
                     haul_proportion_month_yearly := haul_count_per_survey_year_month/total_hauls_survey_year][, 

                     haul_count_per_survey_year_quarter :=uniqueN(haul_id),.(year, survey_unit, quarter)][, #count # hauls per survey unit, year, and month
                     #proportion of hauls for each survey unit and month divided by total # hauls within a survey unit within a year
                     haul_proportion_quarter_yearly := haul_count_per_survey_year_quarter/total_hauls_survey_year] 

FishGlob.uniquehauls.season <- unique(FishGlob.uniquehauls[,.(survey, month, season, haul_counts_per_survey_season_month,total_hauls_survey, haul_proportion_survey_season)]) #rel sampling by season across all years

FishGlob.uniquehauls.quarter <- unique(FishGlob.uniquehauls[,.(survey, month, quarter, haul_counts_per_survey_quarter_month,total_hauls_survey, haul_proportion_survey_quarter)]) #reel sampling by quarter across all years

FishGlob.uniquehauls.annual.month <- unique(FishGlob.uniquehauls[,.(survey, year, survey_unit, month, haul_count_per_survey_year_month,total_hauls_survey_year,haul_proportion_month_yearly)]) #relative sampling by month within years

FishGlob.uniquehauls.annual.quarter <- unique(FishGlob.uniquehauls[,.(survey, year, survey_unit, quarter, haul_count_per_survey_year_quarter,total_hauls_survey_year,haul_proportion_quarter_yearly)]) #relative sampling by month within years

#how does #hauls vary with season and month?
survey_season_month_hauls <- ggplot(FishGlob.10year.spp.uniquehauls.season) +
  geom_tile(aes(x = factor(month), y = factor(season), fill = haul_proportion_survey_season),color = "white") +
  scale_fill_viridis() +
  labs(x = "Month", y = "Season",fill = "Proportion of All Survey Hauls in FishGlob") +
  facet_wrap(~survey,scales = "free_y") +
  theme_classic()

ggsave(survey_season_month_hauls, filename = "survey_season_month_hauls.pdf",path = here::here("figures","view_data"), height = 5, width = 15, units = "in")

#how does #hauls vary with quarter and month?
survey_quarter_month_hauls <- ggplot(FishGlob.uniquehauls.quarter) +
  geom_tile(aes(x = factor(month), y = factor(quarter), fill = haul_proportion_survey_quarter),color = "white") +
  scale_fill_viridis() +
  labs(x = "Month", y = "Quarter",fill = "Proportion of All Survey Hauls in FishGlob") +
  facet_wrap(~survey,scales = "free_y") +
  theme_classic()

ggsave(survey_quarter_month_hauls, filename = "survey_quarter_month_hauls.pdf",path = here::here("figures","view_data"), height = 5, width = 15, units = "in")

#how does #hauls vary with year and month?
year_survey_month_hauls <- ggplot(FishGlob.uniquehauls.annual.month) +
  geom_tile(aes(x = year, y = factor(month), fill = haul_proportion_month_yearly),color = "white") +
  scale_fill_viridis() +
  labs(x = "Year", y = "Month",fill = "Proportion of Annual Hauls") +
  facet_wrap(~survey_unit,scales = "free_y") +
  theme_classic()

ggsave(year_survey_month_hauls, filename = "year_survey_month_hauls.pdf",path = here::here("figures","view_data"), height = 8, width = 16, units = "in")
ggsave(year_survey_month_hauls, filename = "year_survey_month_hauls.pdf",path = here::here("figures","view_data"), height = 8, width = 16, units = "in")

#how does #hauls vary with year and month?
year_survey_quarter_hauls <- ggplot(FishGlob.uniquehauls.annual.quarter) +
  geom_tile(aes(x = year, y = factor(quarter), fill = haul_proportion_quarter_yearly),color = "white") +
  scale_fill_viridis() +
  labs(x = "Year", y = "Quarter",fill = "Proportion of Annual Hauls") +
  facet_wrap(~survey_unit,scales = "free_y") +
  theme_classic()

ggsave(year_survey_quarter_hauls, filename = "year_survey_quarter_hauls.pdf",path = here::here("figures","view_data"), height = 8, width = 16, units = "in")
ggsave(year_survey_quarter_hauls, filename = "year_survey_quarter_hauls.pdf",path = here::here("figures","view_data"), height = 8, width = 16, units = "in")
```

Now, let's look at how location of sampling varies by month of sampling and year of sampling 


```{r location by year plots}
location_by_year <- ggplot(FishGlob.uniquehauls) +
  geom_point(aes(x = longitude_adj, y = latitude, color = year), size = 0.3, alpha = 0.5) +
  scale_color_viridis() +
  facet_wrap(~survey_unit, scales = "free") +
  theme_classic()

ggsave(location_by_year, filename = "location_by_year.pdf",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")

ggsave(location_by_year, filename = "location_by_year.jpg",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")

ggsave(location_by_year, filename = "location_by_year.eps",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")
```


```{r location by month plots}
location_by_month <- ggplot(FishGlob.uniquehauls) +
  geom_point(aes(x = longitude_adj, y = latitude, color = month), size = 0.3, alpha = 0.5) +
  scale_color_viridis(option = "plasma") +
  facet_wrap(~survey_unit, scales = "free") +
  theme_classic()

ggsave(location_by_month, filename = "location_by_month.pdf",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")

ggsave(location_by_month, filename = "location_by_month.jpg",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")

ggsave(location_by_month, filename = "location_by_month.eps",path = here::here("figures","view_data"), height = 8, width = 12, units = "in")
```




###Because time is an essential component of these analyses, we will get rid of any survey x season combinations that are not sampled for at least 10 years

NB: *NS_IBTS (North Sea) is best examined only for 1st and 3rd quarter (pers communication, Aurore Maureaud)

```{r summary by survey region}
#new row for total number of years sampled
FishGlob_1.5[,years_sampled := length(unique(year)),.(survey_unit)]

hist(FishGlob_1.5$years_sampled)
summary(FishGlob_1.5$years_sampled) #ranges from 2 (DFO Straight of Georgia) to 57 (Northeast US)

#statistics about full dataset
nrow(FishGlob_1.5) #4,448,187 total observations
length(unique(FishGlob_1.5[,survey])) #45 regions
length(unique(FishGlob_1.5[,survey_unit])) #54 unique survey/season combinations

#remove observations for any regions x season combinations sampled less than 10 times
FishGlob.10year <- FishGlob_1.5[years_sampled >= 10,]

#statistics about reduced 10 year dataset
nrow(FishGlob.10year) #4,364,328 total observations (83,859 fewer)
length(unique(FishGlob.10year[,survey])) #38 regions (7 fewer)
length(unique(FishGlob.10year[,as.character(survey_unit)])) #46 unique survey/season (8 fewer) combinations

#remove full database
rm(FishGlob_1.5)

#vector with all survey names
all_survey_seasons <- sort(unique(FishGlob.10year[,survey_unit]))

```

###For taxonomic analyses, resolution to species is required. Therefore, we will  exclude any observations not resolved to species. 

```{r spp ID only}
FishGlob.10year.spp <- FishGlob.10year[rank == "Species",] #4,110,764 total observations (253,564 fewer)

#remove full species database
rm(FishGlob.10year)
```


Survey specific changes (These have not actually been implemented as of February 6 2022; standardizations for time and space should catch these)


From Batt et al. 2017
"In the Eastern Bering Sea, sampling years prior to 1984 (data begin in 1982) were excluded from analysis due to large apparent increases in the number of species recorded in the first two years."

"In the Gulf of Mexico, we restricted our analysis to data from 1984 - 2000 (full range
1982-2014); if all years had been used, the number of sites sampled in at least 85% of years would drop from 39 to 13."

"In the Southeast U.S., data from 1989 (data begin in 1989) were excluded because several sites were not sampled in this year, and if this year had been used, the number of sites sampled in at least 85% of years would drop from 24 to 23 (with only 21 sites sampled in 1989)."

"In the Northeast U.S., we excluded data from years prior to 1982 (data begin in 1968). Years prior to 1979 were excluded because strata in the southern tip of the region (between approximately 34.5o N and 35o N) were not regularly sampled during this time. A site in the Gulf of Maine (-69.25o E 43.25o N) was not sampled consistently between 1979 and 1981, and including these years in the analysis would have prevented this site from being included in the analysis (which would have reduced total sites from 100 to 99)." #Alexa says start in 1968/1972

"In Newfoundland, years prior to 1996 (first year was 1992) were excluded because many sites were not sampled. If all years had been used (1992 onward), total number of sites sampled in at least 85% of years would have been decreased from 191 to 53; if data from 1995 onward had been included, number of sites would have been 179." 

Maybe throw out last year of triannual survey for West Coast US (overlaps with first)

(Skipping for now, hopefully our cleaning procedure will cover our bases here)

###Standardize observations for all regions

(This is preliminary code where we use threshold of 70% observation coverage for getting rid of years and n = 1/year for keeping cells)

At some point in the near future, I will just use Laura's code here, or perhaps her final data product where we eliminate observations that don't match consistent spatial footprint but limit whether this happens in years  or locations by # of observations lost. 

####Edit function to create grid
From [here](https://strimas.com/post/hexagonal-grids/)

We will use cell size of 7,774.2 km^2, as that will match grid cell size of 8 in dggridr. We can't use the package dggridr unfortunately because it doesn't work for this version of R (and others have had this issue too). https://github.com/r-barnes/dggridR
For Norway (?) we will use cell size of 23,322.2 km^2 because the sites are further away from each other.

Make sampling locations into spatial points
```{r}
#delete if NA for longitude or latitude
FishGlob.10year.spp <- FishGlob.10year.spp[complete.cases(FishGlob.10year.spp[,.(longitude, latitude)])] #4,110,727 observations, loss of 37

```

Match lat/lon sampling points to hexagonal cells, so that we can see how many cells to keep to maintain a lot of observation points

```{r match lat lon to hex cells}
FishGlob.10year.spp.cells <- data.table()

#two potential cell sizes
  #set cell area (depends on whether or not it's Norway)
  cell_area_all  <- 7774.2 #km2 (8 from dggrdr)
  cell_area_norway <- 23322.2 #km2 (7 from dggrdr; if you want to use different resolution, not doing as of now)
  

for(i in 1:length(all_survey_seasons)){
  FishGlob.10year.spp_subset <- FishGlob.10year.spp[survey_unit == all_survey_seasons[i],]
  
  #unique lat lon combos
  FishGlob.10year.spp_subset_unique <- unique(FishGlob.10year.spp_subset[,.(longitude,latitude,haul_id,year)])
  
  #coordinates to Spatial Points Object
  if(max(FishGlob.10year.spp_subset_unique[,longitude]) - min(FishGlob.10year.spp_subset_unique[,longitude]) > 359){ #if survey region crosses dateline, use st_shift_longitude()
    sp <- FishGlob.10year.spp_subset_unique %>%
          st_as_sf(coords = c("longitude","latitude"), crs = 4326) %>%
             st_shift_longitude()
  }else{
    sp <- FishGlob.10year.spp_subset_unique %>%
          st_as_sf(coords = c("longitude","latitude"), crs = 4326)
  }
  
  sp.t <- as(sp, "Spatial")
  
  proj4string(sp.t) <- CRS("+proj=longlat")
  
proj <-  ifelse(max(FishGlob.10year.spp_subset_unique[,longitude]) - min(FishGlob.10year.spp_subset_unique[,longitude]) > 359, #if survey region crosses dateline, use +lon_0=-140 instead of +lon_0=0
         "+proj=robin +lon_0=-140 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs",
         "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs")
  
  
  sp.p <- spTransform(sp.t, CRS(proj)) #note km^2 units
  
  #use Concaveman to convert points to polygon
  polygon  <- concaveman(sp, 2, 3)

  polygon_spapol <- as(polygon, "Spatial") #convert simple polygon feature to spatial polygon
  
  proj4string(polygon_spapol) <- CRS("+proj=longlat")
  
  polygon_spapol.p <- spTransform(polygon_spapol, CRS(proj)) #note km^2 units
  
  #create grid 
  #set cell_area based on whether or not it's the Norway survey
 # cell_area_km <- ifelse(all_survey_seasons[i] == "Nor-BTS", cell_area_norway, cell_area_all) #note this is in kilometers (if you want to use different cell resolution for Norway, not doing as of now)
  cell_area_km = cell_area_all
  
  #calculate cell_diameter of hexagons from cell_areas
  cell_diameter_km <- sqrt(2 * cell_area_km / sqrt(3)) # in meters
  

  ext <- as(extent(polygon_spapol.p)
            + 2*cell_diameter_km #add a buffer to make sure all observations are assigned a cell
             , "SpatialPolygons")
 # plot(ext)
 # plot(sp.p, add = T, pch = ".")
  
  projection(ext) <- projection(polygon_spapol.p) #match projection
  
  # generate array of hexagon centers
  g <- spsample(ext, type = "hexagonal", cellsize = cell_diameter_km, offset = c(0.5, 0.5))
  
    # convert center points to hexagons
  g <- HexPoints2SpatialPolygons(g, dx = cell_diameter_km)
  
 plot(g)
 plot(sp.p, add = T, pch = ".")
 title(paste0(all_survey_seasons[i]))

  #link lat lon to cell#
    #where do they overlap
    sp.p$cell_ID <- over(sp.p,g) #over(x=location of queries, y = layer from which geometries are queried)
    
    #link lat long to cell #s
    FishGlob.10year.spp_subset_unique[,cell_ID := sp.p$cell_ID][,cell_year_count := .N, .(cell_ID, year)]
    
    #link back to subsetted database of observations
    FishGlob.10year.spp_subset.cells <- FishGlob.10year.spp_subset[FishGlob.10year.spp_subset_unique, on = c("longitude", "latitude","year","haul_id")]
    
    FishGlob.10year.spp.cells <- rbind(FishGlob.10year.spp.cells, FishGlob.10year.spp_subset.cells)

    
#make sure all projections match for binding of polygons
polygon_spapol.forbind <- spTransform(polygon_spapol.p,
                                      CRS=CRS( "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs"))

polygon_spapol.forbind$survey_unit <- all_survey_seasons[i]
  
#bind polygons into spdf
if(i ==1){
  all_survey_seasons_polygon <- polygon_spapol.forbind #if first, just polygon to start
}else{
  all_survey_seasons_polygon <- rbind(all_survey_seasons_polygon, polygon_spapol.forbind)  #if not first, bind new polygon to first polygon
}

   
  
}
  
writeOGR(all_survey_seasons_polygon, dsn = here::here("output/shapefiles"))
```
Animate changes in sampling sites over time for each region in order to identify any huge issues
```{r}
library(gapminder)
library(gganimate)
library(gifski)
```




Merge all shapefiles together in order to send to Sea Around Us team to obtain fishing specific fishing data.
```{r}

```


________

Standardize observations by # of hauls per cell and # cells sampled per year

Remove any years that sample less than 85% of cells  ever sampled
Remove any cells  that are sampled in less than 85% of years

```{r standardize observations all regions}

#account of data loss by standardization
data_loss <- data.table(survey_unit=all_survey_seasons,
                        year_threshold=as.numeric(NA),
                        cell_threshold=as.numeric(NA),
                        percent_years_excluded=as.numeric(NA),
                        percent_hauls_excluded_by_year=as.numeric(NA),
                        percent_obs_excluded_by_year=as.numeric(NA),
                        percent_cells_excluded=as.numeric(NA),
                        percent_hauls_excluded_total=as.numeric(NA),
                        percent_obs_excluded_total=as.numeric(NA))

FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete <- data.table()

for (i in 1:length(all_survey_seasons)) {
      
      #subset to region
      FishGlob.10year.spp.cells_subset <- FishGlob.10year.spp.cells[survey_unit == all_survey_seasons[i],]
      
      #unique year, #cells  sampled
      year_cells_sampled <- unique(FishGlob.10year.spp.cells_subset[,.(year,cell_ID)])
      year_cells_sampled <- year_cells_sampled[,yearly_cell_count := .N,year]
      
      #we'll make benchmark 70% just for now
      year_benchmark <- 0.70
      benchmark_value <- year_benchmark*max(year_cells_sampled[,yearly_cell_count])
      
      #only keep years where over 70% of cells are sampled
      year_cells_sampled[,benchmark := yearly_cell_count >= benchmark_value]
      
      years_deleted <- unique(year_cells_sampled[benchmark == F,year]) #which years are left out?
      
      years_kept <-unique(year_cells_sampled[benchmark ==T,year]) #which years to keep
      
      years_deleted_percent <- round(length(years_deleted)/length(unique(year_cells_sampled[,year]))*100,1)
      

       #print the years that are left out
      print(ifelse(length(years_deleted) == 0, paste0(all_survey_seasons[i], " Years left out = 0"), paste0(all_survey_seasons[i], " Years left out = ", years_deleted, collapse = ",")))
     
     print(paste0(years_deleted_percent, "% of Years Excluded"))
      
      #reduce to years that are well sampled
      FishGlob.10year.spp.cells_subset.wellsampledyears <- FishGlob.10year.spp.cells_subset[year %in% years_kept,]
      
      #how many observations does this remove?
      percent_obs_removed_year <- round((nrow(FishGlob.10year.spp.cells_subset)-nrow(FishGlob.10year.spp.cells_subset.wellsampledyears))/nrow(FishGlob.10year.spp.cells_subset)*100,2)
      
      percent_hauls_removed_year <- round((length(unique(FishGlob.10year.spp.cells_subset[,haul_id]))-length(unique(FishGlob.10year.spp.cells_subset.wellsampledyears[,haul_id])))/length(unique(FishGlob.10year.spp.cells_subset[,haul_id]))*100,2)
      
      #identify any cells that are not sampled in 85% of years
      FishGlob.10year.spp.cells_subset.wellsampledyears[,year_cell_count := length(unique(haul_id)),.(year,cell_ID)] # unique haul ids per cell per year
      
      cell_by_year <- unique(FishGlob.10year.spp.cells_subset.wellsampledyears[, .(cell_ID,year)])
      
      cell_by_year[,years_per_cell := .N,cell_ID]
      
      #cell ids to remove and keep
      #in any year, which cells are sampled in less than 70% of years
      #we'll make benchmark 70% just for now
      cell_benchmark <- 0.70
      benchmark_value_year_count <- cell_benchmark*max(cell_by_year[,years_per_cell])
      
      cell_id_remove <- unique(cell_by_year[years_per_cell<benchmark_value_year_count,cell_ID])
      
      cells_deleted_percent <- round(length(cell_id_remove)/length(unique(FishGlob.10year.spp.cells_subset.wellsampledyears[,cell_ID]))*100,1)
      #this removes 40% of cells from Aleutian islands, seems like too much, I will return to this
      
      #reduce to cells that are well sampled
      FishGlob.10year.spp.cells_subset.wellsampledyearscells <- FishGlob.10year.spp.cells_subset.wellsampledyears[!cell_ID %in% cell_id_remove,]
      
      #add to cleaned data table of all regions
      FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete  <- rbind(FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete, FishGlob.10year.spp.cells_subset.wellsampledyearscells)
      
      #What percent of hauls does this remove?
      hauls_removed_yearcell <- round((length(unique(FishGlob.10year.spp.cells_subset[,haul_id]))-length(unique(FishGlob.10year.spp.cells_subset.wellsampledyearscells[,haul_id])))/length(unique(FishGlob.10year.spp.cells_subset[,haul_id]))*100,1) 
      
      #What percent of observations does this remove?
      obs_removed_yearcell <- round((nrow(FishGlob.10year.spp.cells_subset)-nrow(FishGlob.10year.spp.cells_subset.wellsampledyearscells))/nrow(FishGlob.10year.spp.cells_subset)*100,1)
      
      cell_id_remove.string <- paste(cell_id_remove, collapse = ", ")
      obs_removed.string <- paste(obs_removed_yearcell, collapse = ", ")
      
      #build data table from this reduced output
      FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete <- rbind(FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete, FishGlob.10year.spp.cells_subset.wellsampledyearscells)
      
      #fill out table with statistics of dropped observations
      data_loss[i, "year_threshold"] = year_benchmark
      data_loss[i, "cell_threshold"] = cell_benchmark
      data_loss[i, "percent_years_excluded"] = years_deleted_percent
      data_loss[i, "percent_hauls_excluded_by_year"] = percent_hauls_removed_year
      data_loss[i, "percent_obs_excluded_by_year"] = percent_obs_removed_year
      data_loss[i, "percent_cells_excluded"] = cells_deleted_percent
      data_loss[i, "percent_hauls_excluded_total"] = hauls_removed_yearcell
      data_loss[i, "percent_obs_excluded_total"] = obs_removed_yearcell
      
        #print portion of cells that are left out
      print(ifelse(length(cell_id_remove) == 0, paste0(all_survey_seasons[i], " Cells left out = 0"), paste0(all_survey_seasons[i], " Cells left out = ", cell_id_remove.string, ", ",cells_deleted_percent, "% Cells Excluded, ",hauls_removed_yearcell,"% Hauls Removed, ", obs_removed_yearcell, "% Observations Removed")))
      
      }
     


#now, check again to see if any are less than 10 years
FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete[,years_sampled_update := length(unique(year)),.(survey_unit)]
FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete.10year <- FishGlob.10year.spp.cells_subset.wellsampledyearscells_complete[years_sampled_update >= 10,]

#saveRDS(FishGlob_cleaned.10year, here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.rds"))

#FishGlob_cleaned.10year <- readRDS(here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.rds"))


```

Plot unique trawl areas
```{r plot glob with points}
#pull points
FishGlob_cleaned.10year.lat.lon <- unique(FishGlob_cleaned.10year[,.(longitude,latitude,survey)])

FishGlob_cleaned.10year.lat.lon.spdf <- SpatialPointsDataFrame(coords = FishGlob_cleaned.10year.lat.lon[,1:2], data = FishGlob_cleaned.10year.lat.lon,
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))

#ddgrdr grid for all points of tows that are used
FishGlob_cleaned.10year.lat.lon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells

#centersof cells
FishGlob_cleaned.10year.lat.lon_cellcenters <- dgSEQNUM_to_GEO(dggs, FishGlob_cleaned.10year.lat.lon[,cell])

#linking cell centers to unique_EBS_latlon
FishGlob_cleaned.10year.lat.lon[,LON_CENTER := FishGlob_cleaned.10year.lat.lon_cellcenters$lon_deg][,LAT_CENTER := FishGlob_cleaned.10year.lat.lon_cellcenters$lat_deg]

#get the grid cell boundary for cells which had trawls
grid_FishGlob_cleaned.10year.lat.lon <- dgcellstogrid(dggs, FishGlob_cleaned.10year.lat.lon[,cell], frame = T, wrapcells = F)

#update grid properties to include # of trawls in each cell
grid_FishGlob_cleaned.10year.lat.lon <- merge(grid_FishGlob_cleaned.10year.lat.lon, FishGlob_cleaned.10year.lat.lon, by = "cell")

world <- ne_countries(scale = "medium", returnclass = "sf") #set up for world map
class(world)

global_grids <- ggplot(data = world) +
    geom_sf(fill = "black", color = NA) +
  geom_point(data = FishGlob_cleaned.10year.lat.lon, aes(x = longitude, y = latitude, color = survey), shape = 20, size = 0.000000001) + 
#  geom_polygon(grid_FishGlob_cleaned.10year.lat.lon, mapping = aes(x = long,y = lat, group = cell), inherit.aes = FALSE, fill = NA, color = "darkgrey", size = 0.1) +
  theme_classic() +  theme(legend.position = "none")




#save global map
ggsave(global_grids, path = here::here("figures","map_points_plots"), filename = "global_grids.jpg", height = 8, width = 12)
```

##Sensitivity Analyses (March 2022 @ FishGlob Montpellier)

Just year threshold sensitivity
```{r year threshold sensitivity}
#
#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least n observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

percent_thresholds <- seq(0.5, 1, by = 0.01)

#empty table with sensitivity statistics

year_threshold_sensitivity_full <- data.table(matrix(ncol = 6))

colnames(year_threshold_sensitivity_full) <-  c("survey_season","percent_threshold","years_deleted_percent","years_deleted_count", "obs_deleted_percent", "obs_deleted_count")

#leave out ZAF_1 and MEDITS_2 because they fail at GEO_to_SEQNUM (fix latere)
leave_out_error <- c("ZAF_1", "MEDITS_2")

all_survey_seasons <- all_survey_seasons[!(all_survey_seasons %in% leave_out_error)]

for (i in 1:length(all_survey_seasons)) {
      
      #reduce to specific survey/season combination
      reduced_FishGlob.10year <- FishGlob.10year[survey_season == all_survey_seasons[i],]
      
      #pull out unique lat lons
      unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude)])
      
      unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for this region/survey combo
    
      #find cell centers
      cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell]) #check, fails for MEDITS_2 and ZAF_1
    
      #linking cell centers to unique_latlon
      unique_latlon[,cell_center_longitude := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]
    
        #link centers back to main data table
      reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude"))
    
      #number of tows in each cell
      towcount <- unique_latlon[, .N, by = cell]
    
      #get the grid cell boundary for cells which had trawls
      grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)
    
      #update grid properties to include # of trawls in each cell
      grid <- merge(grid, unique_latlon, by = "cell")
      
      #Any years where clearly fewer cells were sampled?
      year_cells <- reduced_FishGlob.10year.gridded[,.(cell_count = length(unique(cell))),year]
      
      for(j in 1:length(percent_thresholds)) {
      
      benchmark <- percent_thresholds[j] * max(year_cells[,cell_count]) # of cells/ year to cut off below
      
     # assign(paste0("benchmark_",percent_thresholds[j]*100,"%"), benchmark) #unhash if you want to save object
      
      #only keep years where over x% of cells are sampled
      year_cells[,benchmark_met := cell_count > benchmark]
      
      years_deleted <- year_cells[benchmark_met == F]$year #which years are left out?
      
      years_kept <- year_cells[benchmark_met ==T]$year #which years  to keep
      
      years_deleted_percent <- length(years_deleted)/nrow(year_cells)*100
      
      years_deleted_count <- length(years_deleted)
    
      
      #reduce to years that are well sampled
      reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
      
      #identify any cells that in any years are sampled less than 3 times
      reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
      
           #continue to limit by the number of observations for grid cell per year (start with n = 1)
      
            #cell ids to remove and keep
      #in any year, which cells are sampled less than 1 times, these need to go (sensitivity below)
      cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < 1,cell]) 
      
      cells_deleted_percent <- length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))
      #what percent of cells are deleted
      
      cells_deleted_count <- length(cell_id_remove)
      
      #reduce to cells that are well sampled
      reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!(cell %in% cell_id_remove),]
      
      #add to cleaned data table of all regions don't need to do this
#      FishGlob_cleaned_year_sensitivity  <- rbind(FishGlob_cleaned_year_sensitivity, reduced_FishGlob.10year.gridded.r.cell)
      
      #What percent of observations does this remove?
      obs_deleted_percent <- (length(unique(reduced_FishGlob.10year[,haul_id]))-length(unique(reduced_FishGlob.10year.gridded.r.cell[,haul_id])))/length(unique(reduced_FishGlob.10year[,haul_id])) #what % obs do we lose
      
            obs_deleted_count <- length(unique(reduced_FishGlob.10year[,haul_id]))-length(unique(reduced_FishGlob.10year.gridded.r.cell[,haul_id])) #what # obs do we lose
      
      #add to row in small data.table
            
                  year_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], percent_thresholds[j],years_deleted_percent, years_deleted_count, obs_deleted_percent,obs_deleted_count), nrow = 1))
      
      
      year_threshold_sensitivity_full <- rbind(year_threshold_sensitivity_full, year_threshold_sensitivity, use.names = F)
  
      }
      
  print(paste0(all_survey_seasons[i]))
}

#delete first empty row
year_threshold_sensitivity_full <- year_threshold_sensitivity_full[-1,]
year_threshold_sensitivity_full[,percent_threshold := as.numeric(percent_threshold)][,years_deleted_percent := as.numeric(years_deleted_percent)][,years_deleted_count := as.numeric(years_deleted_count)][,obs_deleted_percent := as.numeric(obs_deleted_percent)][,obs_deleted_count := as.numeric(obs_deleted_count)]

#Make plot
year_threshold_sensitivity_plot <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = years_deleted_percent)) +
  geom_line(aes(color = survey_season)) +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
year_threshold_sensitivity_plot_facet <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = years_deleted_percent)) +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
year_threshold_sensitivity_plot_box <- ggplot(year_threshold_sensitivity_full, aes(y = years_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  geom_boxplot() +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

year_threshold_sensitivity_merge <- plot_grid(year_threshold_sensitivity_plot + theme(legend.position = "none"), year_threshold_sensitivity_plot_box + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(year_threshold_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")

#for individual hauls
#Make plot
year_threshold_sensitivity_plot_by_tow <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = 1-obs_deleted_percent)) +
  geom_line(aes(color = survey_season)) +
    labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_by_tow.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
year_threshold_sensitivity_plot_facet_by_tow <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = 1-obs_deleted_percent)) +
  labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_facet_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_facet_by_tow.jpg", unit = "in", width = 3, height = 15)

#Make box plot
year_threshold_sensitivity_plot_box_by_tow <- ggplot(year_threshold_sensitivity_full, aes(y = 1-obs_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
  geom_boxplot() +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_box_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_box_by_tow.jpg", width = 10, height = 5, unit = "in")

year_threshold_sensitivity_merge_by_tow <- plot_grid(year_threshold_sensitivity_plot_by_tow + theme(legend.position = "none"), year_threshold_sensitivity_plot_box_by_tow + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(year_threshold_sensitivity_merge_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_merge_by_tow.jpg", width = 10, height = 5, unit = "in")
```

Just cell count threshold sensitivity (But, should be tows, not observations)

```{r sensitivity analysis}

FishGlob_cleaned_cell_sensitivity <- data.table()

#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least 3 observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

#Keep % cut off at 70%
#Vary cell counts
cell_count_thresholds <- seq(0,10, by = 1)

#sensitivity (what portion years left out, what portion cells left out)
cell_threshold_sensitivity_full <- data.table(matrix(ncol = 6))

colnames(cell_threshold_sensitivity_full) <-  c("survey_season","cell_count_threshold","cells_deleted_percent","cells_deleted_count", "obs_removed_percent","obs_removed_count")

#GSL-S_3 acting weird, leave out #23

for (i in 1:length(all_survey_seasons)) {
      
      #reduce to specific survey/season combination
      reduced_FishGlob.10year <- FishGlob.10year[survey_season == all_survey_seasons[i],]
      
      #pull out unique lat lons
      unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude)])
      
      unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for this region/survey combo
    
      #find cell centers
      cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell])
    
      #linking cell centers to unique_latlon
      unique_latlon[,cell_center_longitude_s := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]
    
        #link centers back to main data table
      reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude"))
    
      #number of tows in each cell
      towcount <- unique_latlon[, .N, by = cell]
    
      #get the grid cell boundary for cells which had trawls
      grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)
    
      #update grid properties to include # of trawls in each cell
      grid <- merge(grid, unique_latlon, by = "cell")
      
      #Any years where clearly fewer cells were sampled?
      year_cells <- reduced_FishGlob.10year.gridded[,.(cell_count = length(unique(cell))),year]
      
      #set year benchmark to 70%
      benchmark <- percent_thresholds[21] * max(year_cells[,cell_count]) # of cells/ year to cut off below
      
     # assign(paste0("benchmark_",percent_thresholds[j]*100,"%"), benchmark) #unhash if you want to save object
      
      #only keep years where over x% of cells are sampled
      year_cells[,benchmark_met := cell_count > benchmark]
      
  #    years_deleted <- year_cells[benchmark_met == F]$year #which years are left out?
      
      years_kept <- year_cells[benchmark_met ==T]$year #which years  to keep
      
  #    years_deleted_percent <- length(years_deleted)/nrow(year_cells)*100
      
  #    years_deleted_count <- length(years_deleted)
      
  #    year_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], percent_thresholds[j],years_deleted_percent, years_deleted_count), nrow = 1))
      
      
   #   year_threshold_sensitivity_full <- rbind(year_threshold_sensitivity_full, year_threshold_sensitivity, use.names = F)
      
      #reduce to years that are well sampled
      reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
      
      #identify any cells that in any years are sampled less than 3 times
      reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
      
      for (k in 1:length(cell_count_thresholds)) {
      #cell ids to remove and keep
      #in any year, which cells are sampled less than x times, these need to go
      cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < cell_count_thresholds[k],cell]) 
      
      cells_deleted_percent <- length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))
      #what percent of cells are deleted
      
      cells_deleted_count <- length(cell_id_remove)
      
      #reduce to cells that are well sampled
      reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!cell %in% cell_id_remove,]
      
      #add to cleaned data table of all regions
      FishGlob_cleaned_cell_sensitivity  <- rbind(FishGlob_cleaned_cell_sensitivity, reduced_FishGlob.10year.gridded.r.cell)
      
      #What percent of observations does this remove?
      obs_removed_percent <- (nrow(reduced_FishGlob.10year.gridded.r)-nrow(reduced_FishGlob.10year.gridded.r.cell))/nrow(reduced_FishGlob.10year.gridded.r) #what % obs do we lose
      
            obs_removed_count <- nrow(reduced_FishGlob.10year.gridded.r)-nrow(reduced_FishGlob.10year.gridded.r.cell) #what # obs do we lose
      
      #add to row in small data.table
           cell_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], cell_count_thresholds[k],cells_deleted_percent, cells_deleted_count, obs_removed_percent, obs_removed_count), nrow = 1))
      
      #combine with full data.table
      cell_threshold_sensitivity_full <- rbind(cell_threshold_sensitivity_full, cell_threshold_sensitivity, use.names = F)
      
  
      }
     print(paste0(all_survey_seasons[i]))
    }

#delete first empty row
cell_threshold_sensitivity_full <- cell_threshold_sensitivity_full[-1,]
cell_threshold_sensitivity_full[,cell_count_threshold := as.numeric(cell_count_threshold)][,cells_deleted_percent := as.numeric(cells_deleted_percent)][,cells_deleted_count := as.numeric(cells_deleted_count)][,obs_removed_percent := as.numeric(obs_removed_percent)][,obs_removed_count := as.numeric(obs_removed_count)]

#Make plot
cell_threshold_sensitivity_plot <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = cells_deleted_percent)) +
  geom_point()
  geom_line(aes(color = survey_season)) +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
cell_threshold_sensitivity_plot_facet <- ggplot(cell_threshold_sensitivity_full, aes(x = percent_threshold, y = cells_deleted_percent)) +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
cell_threshold_sensitivity_plot_box <- ggplot(year_threshold_sensitivity_full, aes(y = cells_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  geom_boxplot() +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

cell_threshold_sensitivity_merge <- plot_grid(cell_threshold_sensitivity_plot + theme(legend.position = "none"), cell_threshold_sensitivity_plot_box + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(cell_threshold_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")


#What about observations instead of cells?
#Make plot
cell_threshold_obs_sensitivity_plot <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = 1-obs_removed_percent)) +
  geom_line(aes(color = survey_season)) +
      labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
cell_threshold_obs_sensitivity_plot_facet <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = 1-obs_removed_percent)) +
  geom_line() +
        labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
cell_threshold_obs_sensitivity_plot_box <- ggplot(cell_threshold_sensitivity_full, aes(y = 1-obs_removed_percent, x = cell_count_threshold, group = cell_count_threshold)) +
        labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
  geom_boxplot() +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

cell_threshold_obs_sensitivity_merge <- plot_grid(cell_threshold_obs_sensitivity_plot + theme(legend.position = "none"), cell_threshold_obs_sensitivity_plot_box  + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(cell_threshold_obs_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")
```