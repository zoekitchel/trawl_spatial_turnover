---
title: "Load Fishglob Database and Apply Temporal and Spatial Standardization"
output: html_notebook
---

This code is Script 1 for Kitchel et al. TITLE manuscript.

- This project is a collaborative effort to describe changes in taxonomic composition  of fish communities around the world--as sampled by bottom trawl surveys.

- Code by ZoÃ« J. Kitchel

SESSION INFO

R version 4.2.1 (2022-06-23)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Big Sur 11.7

Matrix products: default
LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] cowplot_1.1.1     sf_1.0-9          here_1.0.1        googledrive_2.0.0
 [5] geosphere_1.5-14  taxize_0.9.100    rgdal_1.6-2       data.table_1.14.4
 [9] rasterVis_0.51.4  lattice_0.20-45   gridExtra_2.3     viridis_0.6.2    
[13] viridisLite_0.4.1 rgbif_3.7.3       rgeos_0.5-9       raster_3.6-11    
[17] sp_1.5-0          tidyr_1.2.1       dplyr_1.0.10     

loaded via a namespace (and not attached):
 [1] nlme_3.1-157        fs_1.5.2            bold_1.2.0          oai_0.4.0          
 [5] RColorBrewer_1.1-3  httr_1.4.4          rprojroot_2.0.3     bslib_0.4.0        
 [9] tools_4.2.1         utf8_1.2.2          R6_2.5.1            KernSmooth_2.23-20 
[13] DBI_1.1.3           lazyeval_0.2.2      colorspace_2.0-3    tidyselect_1.2.0   
[17] curl_4.3.3          compiler_4.2.1      cli_3.4.1           xml2_1.3.3         
[21] sass_0.4.2          scales_1.2.1        classInt_0.4-8      hexbin_1.28.2      
[25] proxy_0.4-27        stringr_1.4.1       digest_0.6.30       rmarkdown_2.17     
[29] jpeg_0.1-9          pkgconfig_2.0.3     htmltools_0.5.3     fastmap_1.1.0      
[33] rlang_1.0.6         rstudioapi_0.14     httpcode_0.3.0      jquerylib_0.1.4    
[37] generics_0.1.3      zoo_1.8-11          jsonlite_1.8.3      magrittr_2.0.3     
[41] interp_1.1-3        Rcpp_1.0.9          munsell_0.5.0       fansi_1.0.3        
[45] ape_5.6-2           lifecycle_1.0.3     terra_1.6-41        stringi_1.7.8      
[49] whisker_0.4         yaml_2.3.6          plyr_1.8.7          grid_4.2.1         
[53] parallel_4.2.1      crayon_1.5.2        deldir_1.0-6        conditionz_0.1.0   
[57] knitr_1.40          pillar_1.8.1        uuid_1.1-0          codetools_0.2-18   
[61] crul_1.3            glue_1.6.2          evaluate_0.17       latticeExtra_0.6-30
[65] png_0.1-7           vctrs_0.5.0         foreach_1.5.2       gtable_0.3.1       
[69] purrr_0.3.5         reshape_0.8.9       assertthat_0.2.1    cachem_1.0.6       
[73] ggplot2_3.3.6       xfun_0.34           e1071_1.7-12        class_7.3-20       
[77] gargle_1.2.1        tibble_3.1.8        iterators_1.0.14    units_0.8-0 

```{r}
library(dplyr)
library(tidyr)
library(sp)
library(raster)
library(rgeos)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
library(concaveman)
set.seed(1)
```


```{r setup}
#library(dggridR) #no longer works will find alternate method
library(data.table)
library(rgdal)
library(raster)
library(sp)
#library(rnaturalearth)
#library(rnaturalearthdata)
library(rgeos)
library(taxize) #standardizing names
library(geosphere)  #to calculate distance between lat lon of grid cells
library(googledrive)
library(here)
library(sf)
library(cowplot)
```


Pull in compiled and cleaned data from FishGlob, downloaded on November 28, 2022 (V 1.5) at [this link](https://drive.google.com/file/d/1MgXKhmIufUtjE_Y_mWpDeldJKfd7nDEM/view?usp=share_link). This is typically compiled by Aurore Maureaud.

```{r pull in fishglob database}

FishGlob_1.5 <- fread(here::here("data","FISHGLOB_v1.5_clean.csv"))

```

###Because time is an essential component of these analyses, we will get rid of any survey x season combinations that are not sampled for at least 10 years

NB: *NS_IBTS (North Sea) is best examined only for 1st and 3rd quarter (pers communication, Aurore Maureaud)

```{r summary by survey region}
#new row for total number of years sampled
FishGlob_1.5[,years_sampled := length(unique(year)),.(survey_unit)]

hist(FishGlob_1.5$years_sampled)
summary(FishGlob_1.5$years_sampled) #ranges from 2 (DFO Straight of Georgia) to 57 (Northeast US)

#statistics about full dataset
nrow(FishGlob_1.5) #4,448,187 total observations
length(unique(FishGlob_1.5[,survey])) #45 regions
length(unique(FishGlob_1.5[,survey_unit])) #53 unique survey/season combinations

#remove observations for any regions x season combinations sampled less than 10 times
FishGlob.10year <- FishGlob_1.5[years_sampled >= 10,]

#statistics about reduced 10 year dataset
nrow(FishGlob.10year) #4,364,328 total observations (83,859 fewer)
length(unique(FishGlob.10year[,survey])) #38 regions (7 fewer)
length(unique(FishGlob.10year[,as.character(survey_unit)])) #45 unique survey/season (8 fewer) combinations

#remove full database
rm(FishGlob_1.5)

#vector with all survey names
all_survey_seasons <- sort(unique(FishGlob.10year[,survey_unit]))

```

###For taxonomic analyses, resolution to species is required. Therefore, we will  exclude any observations not resolved to species. 

```{r spp ID only}
FishGlob.10year.spp <- FishGlob.10year[rank == "Species",] #4,110,764 total observations (253,564 fewer)

#remove full species database
rm(FishGlob.10year)
```


Survey specific changes (These have not actually been implemented as of February 6 2022; standardizations for time and space should catch these)


From Batt et al. 2017
"In the Eastern Bering Sea, sampling years prior to 1984 (data begin in 1982) were excluded from analysis due to large apparent increases in the number of species recorded in the first two years."

"In the Gulf of Mexico, we restricted our analysis to data from 1984 - 2000 (full range
1982-2014); if all years had been used, the number of sites sampled in at least 85% of years would drop from 39 to 13."

"In the Southeast U.S., data from 1989 (data begin in 1989) were excluded because several sites were not sampled in this year, and if this year had been used, the number of sites sampled in at least 85% of years would drop from 24 to 23 (with only 21 sites sampled in 1989)."

"In the Northeast U.S., we excluded data from years prior to 1982 (data begin in 1968). Years prior to 1979 were excluded because strata in the southern tip of the region (between approximately 34.5o N and 35o N) were not regularly sampled during this time. A site in the Gulf of Maine (-69.25o E 43.25o N) was not sampled consistently between 1979 and 1981, and including these years in the analysis would have prevented this site from being included in the analysis (which would have reduced total sites from 100 to 99)." #Alexa says start in 1968/1972

"In Newfoundland, years prior to 1996 (first year was 1992) were excluded because many sites were not sampled. If all years had been used (1992 onward), total number of sites sampled in at least 85% of years would have been decreased from 191 to 53; if data from 1995 onward had been included, number of sites would have been 179." 

Maybe throw out last year of triannual survey for West Coast US (overlaps with first)

(Skipping for now, hopefully our cleaning procedure will cover our bases here)

###Standardize observations for all regions

(This is preliminary code where we use threshold of 70% observation coverage for getting rid of years and n = 1/year for keeping cells)

At some point in the near future, I will just use Laura's code here, or perhaps her final data product where we eliminate observations that don't match consistent spatial footprint but limit whether this happens in years  or locations by # of observations lost. 

####Edit function to create grid
From [here](https://strimas.com/post/hexagonal-grids/)

We will use cell size of 7,774.2 km^2, as that will match grid cell size of 8 in dggridr. We can't use the package dggridr unfortunately because it doesn't work for this version of R (and others have had this issue too). https://github.com/r-barnes/dggridR
For Norway (?) we will use cell size of 23,322.2 km^2 because the sites are further away from each other.

Make sampling locations into spatial points
```{r}
#delete if NA for longitude or latitude
FishGlob.10year.spp <- FishGlob.10year.spp[complete.cases(FishGlob.10year.spp[,.(longitude, latitude)])] #4,110,727 observations, loss of 37

```

Go region by region
```{r}
for(i in 1:length(all_survey_seasons)){
  FishGlob.10year.spp_subset <- FishGlob.10year.spp[survey_unit == all_survey_seasons[i],]
  
  #unique lat lon combos
  FishGlob.10year.spp_subset_unique <- unique(FishGlob.10year.spp_subset[,.(longitude,latitude)])
  
  #adjust those that cross dateline, if greater than 150, subtract 360
  FishGlob.10year.spp_subset_unique[,longitude_s := ifelse(longitude > 150,(longitude-360),(longitude))]
  
  #coordinates to Spatial Points Object
  sp <- st_as_sf(x = FishGlob.10year.spp_subset_unique, 
                 coords = c("longitude_s","latitude"))

  sp.t <- as(sp, "Spatial")
  
  proj4string(sp.t) <- CRS("+proj=longlat")
  
  sp.p <- spTransform(sp.t, CRS("+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs")) #note km^2 units
  
  #use Concaveman to convert points to polygon
  polygon  <- concaveman(sp, 2, 3)
  
  polygon_spapol <- as(polygon, "Spatial") #convert simple polygon feature to spatial polygon
  
  proj4string(polygon_spapol) <- CRS("+proj=longlat")
  
  polygon_spapol.p <- spTransform(polygon_spapol, CRS("+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs")) #note km^2 units
  
  #create grid 
  #set cell area (depends on whether or not it's Norway)
  cell_area_all  <- 7774.2 #km2
  cell_area_norway <- 23322.2 #km2
  
    #set cell_area based on whether or not it's the Norway survey
  cell_area <- ifelse(all_survey_seasons[i] == "Nor-BTS", cell_area_norway, cell_area_all)
  
  #calculate cell_diameter of hexagons from cell_areas
  cell_diameter <- sqrt(2 * cell_area / sqrt(3)) # in kilometers  

  #set cell_area based on whether or not it's the Norway survey
  cell_area_m <- ifelse(all_survey_seasons[i] == "Nor-BTS", cell_area_norway*1000, cell_area_all*1000)
  
  #calculate cell_diameter of hexagons from cell_areas
  cell_diameter_m <- sqrt(2 * cell_area / sqrt(3)) # in meters

  ext <- as(extent(polygon_spapol.p)
            + cell_diameter
             , "SpatialPolygons")
  plot(ext)
  plot(sp.p, add = T, pch = ".")
  
  projection(ext) <- projection(polygon_spapol.p) #match projection
  
  # generate array of hexagon centers
  g <- spsample(ext, type = "hexagonal", cellsize = cell_diameter, offset = c(0.5, 0.5))
  
    # convert center points to hexagons
  g <- HexPoints2SpatialPolygons(g, dx = cell_diameter)
  
  plot(g)
  plot(sp.p, add = T, pch = ".")

  #link lat lon to cell#
  #where do they overlap
  over(sp.p,g)
  sp.p$cell_ID <- over(sp.p,g)
  
  #link lat long to cell #s
  FishGlob.10year.spp_subset_unique[,cell_ID := sp.p$cell_ID]
  
  
}
```


Apply grid to Spatial Points
```{r}
fishglob_grid_8 <- function(FishGlobTows, cell_area = 7774.2, clip = FALSE)
```


```{r standardize observations all regions}


#world map
world <- data.table(map_data('world'))
world[,long_s := ifelse(long > 150, (long-360),(long))]


#if positive, subtract 360
FishGlob.10year[,longitude_s := ifelse(longitude > 150,(longitude-360),(longitude))]

#delete if NA for longitude or latitude
FishGlob.10year <- FishGlob.10year[complete.cases(FishGlob.10year[,.(longitude, latitude)])]

#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least 1 observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

map_points_plots <- list()

FishGlob_cleaned <- data.table()


for (i in 1:length(all_survey_seasons)) {
      
      #reduce to specific survey/season combination
      reduced_FishGlob.10year <- FishGlob.10year[survey_unit == all_survey_seasons[i],]
      
      #pull out unique lat lons
      unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude_s)])
      
      unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude_s, latitude)] #get corresponding grid cells for this region/survey combo
    
      #find cell centers
      cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell])
    
      #linking cell centers to unique_latlon
      unique_latlon[,cell_center_longitude_s := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]
    
        #link centers back to main data table
      reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude_s"))
    
      #number of tows in each cell
      towcount <- unique_latlon[, .N, by = cell]
    
      #get the grid cell boundary for cells which had trawls
      grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)
    
      #update grid properties to include # of trawls in each cell
      grid <- merge(grid, unique_latlon, by = "cell")
      
      #Any years where clearly fewer haulids were sampled?
      year_haulids <- reduced_FishGlob.10year.gridded[,.(haulid_count = length(unique(haul_id))),year]
      
      #we'll make benchmark 85% just for now
      benchmark_value <- 0.7*max(year_haulids[,haulid_count])
      #only keep years where over 85% of cells are sampled
      year_haulids[,benchmark := haulid_count >= benchmark_value]
      
      years_deleted <- year_haulids[benchmark == F]$year #which years are left out?
      
      years_kept <- year_haulids[benchmark ==T]$year #which years  to keep
      
      years_deleted_percent <- round(length(years_deleted)/nrow(year_haulids)*100,1)
      

       #print the years that are left out
      print(ifelse(length(years_deleted) == 0, paste0(all_survey_seasons[i], " Years left out = 0"), paste0(all_survey_seasons[i], " Years left out = ", years_deleted, collapse = ",")))
     
     print(paste0(years_deleted_percent, "% of Years Excluded"))
      
      #reduce to years that are well sampled
      reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
      
      #identify any cells that in any years are sampled less than 1 times
      reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
      
      #cell ids to remove and keep
      #in any year, which cells are sampled less than 1 times, these need to go
      cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < 1]$cell)
      cells_deleted_percent <- round(length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))*100,1)
      #this removes 40% of cells from Aleutian islands, seems like too much, I will return to this
      
      #reduce to cells that are well sampled
      reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!cell %in% cell_id_remove,]
      
      #add to cleaned data table of all regions
      FishGlob_cleaned  <- rbind(FishGlob_cleaned, reduced_FishGlob.10year.gridded.r.cell)
      
      #What percent of observations does this remove?
      obs_removed <- round((nrow(reduced_FishGlob.10year.gridded.r)-nrow(reduced_FishGlob.10year.gridded.r.cell))/nrow(reduced_FishGlob.10year.gridded.r)*100,1) #we lose 13.4% of observations
      
      cell_id_remove.string <- paste(cell_id_remove, collapse = ", ")
      obs_removed.string <- paste(obs_removed, collapse = ", ")
      
        #print portion of cells that are left out
      print(ifelse(length(cell_id_remove) == 0, paste0(all_survey_seasons[i], " Cells left out = 0"), paste0(all_survey_seasons[i], " Cells left out = ", cell_id_remove.string, ", ",cells_deleted_percent, "% Cells Excluded, ",obs_removed, "% Observations Removed")))
      
      #make a map of these points
      
      #unique lat lon
      unique_latlon_reduced <- unique(reduced_FishGlob.10year.gridded.r.cell[,.(longitude,  latitude)])
      #if greater than 150, subtract 360
      unique_latlon_reduced[,longitude_s := ifelse(longitude > 150,(longitude-360),(longitude))]
    
        #set bounds of basemap using coordinates
      reg <- world[long_s >= min(unique_latlon_reduced[,longitude_s]-2) & long_s <= max(unique_latlon_reduced[,longitude_s]+2) & lat >= min(unique_latlon_reduced[,latitude]-2) & lat <= max(unique_latlon_reduced[,latitude]+2)]
      
      if(max(unique_latlon_reduced[,longitude] > 150)) {
      #make map
      map_points_plots[[i]] <- ggplot() +
    geom_polygon(data = world, aes(x=long_s, y = lat, group = group), 
                                     fill = "black", 
                                     color="black") +
      geom_point(data = unique_latlon_reduced,
        aes(
          x = longitude_s,
          y = latitude),
        color = "red", size = 0.05
      ) +
        labs(x = "Longitude", y = "Latitude") +
        coord_equal(xlim = c(min(unique_latlon_reduced[,longitude_s]-2), max(unique_latlon_reduced[,longitude_s]+2)), 
                    ylim = c(min(unique_latlon_reduced[,latitude]-2), max(unique_latlon_reduced[,latitude]+2))) +
      theme_classic()
      
      mapfilename <- paste0(all_survey_seasons[i], "_map_point_plot.jpg")
    
     ggsave(map_points_plots[[i]], filename = mapfilename, path = here::here("figures","map_points_plots"))
      } else {
          #make map
      map_points_plots[[i]] <- ggplot() +
    geom_polygon(data = world, aes(x=long, y = lat, group = group), 
                                     fill = "black", 
                                     color="black") +
      geom_point(data = unique_latlon_reduced,
        aes(
          x = longitude,
          y = latitude),
        color = "red", size = 0.05
      ) +
        labs(x = "Longitude", y = "Latitude") +
        coord_equal(xlim = c(min(unique_latlon_reduced[,longitude]-2), max(unique_latlon_reduced[,longitude]+2)), 
                    ylim = c(min(unique_latlon_reduced[,latitude]-2), max(unique_latlon_reduced[,latitude]+2))) +
      theme_classic()
      
      mapfilename <- paste0(all_survey_seasons[i], "_map_point_plot.jpg")
    
     ggsave(map_points_plots[[i]], filename = mapfilename, path = here::here("figures","map_points_plots"))
      }
     
    }

#now, check again to see if any are less than 10 years
FishGlob_cleaned[,years_sampled_update := length(unique(year)),.(survey,season)]
FishGlob_cleaned.10year <- FishGlob_cleaned[years_sampled_update >= 10,]

#saveRDS(FishGlob_cleaned.10year, here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.rds"))

#FishGlob_cleaned.10year <- readRDS(here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.rds"))

length(unique(FishGlob_cleaned.10year[,survey]))

```

Plot unique trawl areas
```{r plot glob with points}
#pull points
FishGlob_cleaned.10year.lat.lon <- unique(FishGlob_cleaned.10year[,.(longitude,latitude,survey)])

FishGlob_cleaned.10year.lat.lon.spdf <- SpatialPointsDataFrame(coords = FishGlob_cleaned.10year.lat.lon[,1:2], data = FishGlob_cleaned.10year.lat.lon,
                               proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))

#ddgrdr grid for all points of tows that are used
FishGlob_cleaned.10year.lat.lon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells

#centersof cells
FishGlob_cleaned.10year.lat.lon_cellcenters <- dgSEQNUM_to_GEO(dggs, FishGlob_cleaned.10year.lat.lon[,cell])

#linking cell centers to unique_EBS_latlon
FishGlob_cleaned.10year.lat.lon[,LON_CENTER := FishGlob_cleaned.10year.lat.lon_cellcenters$lon_deg][,LAT_CENTER := FishGlob_cleaned.10year.lat.lon_cellcenters$lat_deg]

#get the grid cell boundary for cells which had trawls
grid_FishGlob_cleaned.10year.lat.lon <- dgcellstogrid(dggs, FishGlob_cleaned.10year.lat.lon[,cell], frame = T, wrapcells = F)

#update grid properties to include # of trawls in each cell
grid_FishGlob_cleaned.10year.lat.lon <- merge(grid_FishGlob_cleaned.10year.lat.lon, FishGlob_cleaned.10year.lat.lon, by = "cell")

world <- ne_countries(scale = "medium", returnclass = "sf") #set up for world map
class(world)

global_grids <- ggplot(data = world) +
    geom_sf(fill = "black", color = NA) +
  geom_point(data = FishGlob_cleaned.10year.lat.lon, aes(x = longitude, y = latitude, color = survey), shape = 20, size = 0.000000001) + 
#  geom_polygon(grid_FishGlob_cleaned.10year.lat.lon, mapping = aes(x = long,y = lat, group = cell), inherit.aes = FALSE, fill = NA, color = "darkgrey", size = 0.1) +
  theme_classic() +  theme(legend.position = "none")




#save global map
ggsave(global_grids, path = here::here("figures","map_points_plots"), filename = "global_grids.jpg", height = 8, width = 12)
```

##Sensitivity Analyses (March 2022 @ FishGlob Montpellier)

Just year threshold sensitivity
```{r year threshold sensitivity}
#
#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least n observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

percent_thresholds <- seq(0.5, 1, by = 0.01)

#empty table with sensitivity statistics

year_threshold_sensitivity_full <- data.table(matrix(ncol = 6))

colnames(year_threshold_sensitivity_full) <-  c("survey_season","percent_threshold","years_deleted_percent","years_deleted_count", "obs_deleted_percent", "obs_deleted_count")

#leave out ZAF_1 and MEDITS_2 because they fail at GEO_to_SEQNUM (fix latere)
leave_out_error <- c("ZAF_1", "MEDITS_2")

all_survey_seasons <- all_survey_seasons[!(all_survey_seasons %in% leave_out_error)]

for (i in 1:length(all_survey_seasons)) {
      
      #reduce to specific survey/season combination
      reduced_FishGlob.10year <- FishGlob.10year[survey_season == all_survey_seasons[i],]
      
      #pull out unique lat lons
      unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude)])
      
      unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for this region/survey combo
    
      #find cell centers
      cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell]) #check, fails for MEDITS_2 and ZAF_1
    
      #linking cell centers to unique_latlon
      unique_latlon[,cell_center_longitude := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]
    
        #link centers back to main data table
      reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude"))
    
      #number of tows in each cell
      towcount <- unique_latlon[, .N, by = cell]
    
      #get the grid cell boundary for cells which had trawls
      grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)
    
      #update grid properties to include # of trawls in each cell
      grid <- merge(grid, unique_latlon, by = "cell")
      
      #Any years where clearly fewer cells were sampled?
      year_cells <- reduced_FishGlob.10year.gridded[,.(cell_count = length(unique(cell))),year]
      
      for(j in 1:length(percent_thresholds)) {
      
      benchmark <- percent_thresholds[j] * max(year_cells[,cell_count]) # of cells/ year to cut off below
      
     # assign(paste0("benchmark_",percent_thresholds[j]*100,"%"), benchmark) #unhash if you want to save object
      
      #only keep years where over x% of cells are sampled
      year_cells[,benchmark_met := cell_count > benchmark]
      
      years_deleted <- year_cells[benchmark_met == F]$year #which years are left out?
      
      years_kept <- year_cells[benchmark_met ==T]$year #which years  to keep
      
      years_deleted_percent <- length(years_deleted)/nrow(year_cells)*100
      
      years_deleted_count <- length(years_deleted)
    
      
      #reduce to years that are well sampled
      reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
      
      #identify any cells that in any years are sampled less than 3 times
      reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
      
           #continue to limit by the number of observations for grid cell per year (start with n = 1)
      
            #cell ids to remove and keep
      #in any year, which cells are sampled less than 1 times, these need to go (sensitivity below)
      cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < 1,cell]) 
      
      cells_deleted_percent <- length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))
      #what percent of cells are deleted
      
      cells_deleted_count <- length(cell_id_remove)
      
      #reduce to cells that are well sampled
      reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!(cell %in% cell_id_remove),]
      
      #add to cleaned data table of all regions don't need to do this
#      FishGlob_cleaned_year_sensitivity  <- rbind(FishGlob_cleaned_year_sensitivity, reduced_FishGlob.10year.gridded.r.cell)
      
      #What percent of observations does this remove?
      obs_deleted_percent <- (length(unique(reduced_FishGlob.10year[,haul_id]))-length(unique(reduced_FishGlob.10year.gridded.r.cell[,haul_id])))/length(unique(reduced_FishGlob.10year[,haul_id])) #what % obs do we lose
      
            obs_deleted_count <- length(unique(reduced_FishGlob.10year[,haul_id]))-length(unique(reduced_FishGlob.10year.gridded.r.cell[,haul_id])) #what # obs do we lose
      
      #add to row in small data.table
            
                  year_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], percent_thresholds[j],years_deleted_percent, years_deleted_count, obs_deleted_percent,obs_deleted_count), nrow = 1))
      
      
      year_threshold_sensitivity_full <- rbind(year_threshold_sensitivity_full, year_threshold_sensitivity, use.names = F)
  
      }
      
  print(paste0(all_survey_seasons[i]))
}

#delete first empty row
year_threshold_sensitivity_full <- year_threshold_sensitivity_full[-1,]
year_threshold_sensitivity_full[,percent_threshold := as.numeric(percent_threshold)][,years_deleted_percent := as.numeric(years_deleted_percent)][,years_deleted_count := as.numeric(years_deleted_count)][,obs_deleted_percent := as.numeric(obs_deleted_percent)][,obs_deleted_count := as.numeric(obs_deleted_count)]

#Make plot
year_threshold_sensitivity_plot <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = years_deleted_percent)) +
  geom_line(aes(color = survey_season)) +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
year_threshold_sensitivity_plot_facet <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = years_deleted_percent)) +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
year_threshold_sensitivity_plot_box <- ggplot(year_threshold_sensitivity_full, aes(y = years_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  geom_boxplot() +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

year_threshold_sensitivity_merge <- plot_grid(year_threshold_sensitivity_plot + theme(legend.position = "none"), year_threshold_sensitivity_plot_box + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(year_threshold_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")

#for individual hauls
#Make plot
year_threshold_sensitivity_plot_by_tow <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = 1-obs_deleted_percent)) +
  geom_line(aes(color = survey_season)) +
    labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_by_tow.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
year_threshold_sensitivity_plot_facet_by_tow <- ggplot(year_threshold_sensitivity_full, aes(x = percent_threshold, y = 1-obs_deleted_percent)) +
  labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_facet_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_facet_by_tow.jpg", unit = "in", width = 3, height = 15)

#Make box plot
year_threshold_sensitivity_plot_box_by_tow <- ggplot(year_threshold_sensitivity_full, aes(y = 1-obs_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  labs(x = "Percent Threshold", y = "Percent Hauls Maintained") +
  geom_boxplot() +
  theme_classic()

ggsave(year_threshold_sensitivity_plot_box_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_plot_box_by_tow.jpg", width = 10, height = 5, unit = "in")

year_threshold_sensitivity_merge_by_tow <- plot_grid(year_threshold_sensitivity_plot_by_tow + theme(legend.position = "none"), year_threshold_sensitivity_plot_box_by_tow + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(year_threshold_sensitivity_merge_by_tow, path = here::here("figures","sensitivity"),filename = "year_threshold_sensitivity_merge_by_tow.jpg", width = 10, height = 5, unit = "in")
```

Just cell count threshold sensitivity (But, should be tows, not observations)

```{r sensitivity analysis}

FishGlob_cleaned_cell_sensitivity <- data.table()

#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least 3 observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

#Keep % cut off at 70%
#Vary cell counts
cell_count_thresholds <- seq(0,10, by = 1)

#sensitivity (what portion years left out, what portion cells left out)
cell_threshold_sensitivity_full <- data.table(matrix(ncol = 6))

colnames(cell_threshold_sensitivity_full) <-  c("survey_season","cell_count_threshold","cells_deleted_percent","cells_deleted_count", "obs_removed_percent","obs_removed_count")

#GSL-S_3 acting weird, leave out #23

for (i in 1:length(all_survey_seasons)) {
      
      #reduce to specific survey/season combination
      reduced_FishGlob.10year <- FishGlob.10year[survey_season == all_survey_seasons[i],]
      
      #pull out unique lat lons
      unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude)])
      
      unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for this region/survey combo
    
      #find cell centers
      cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell])
    
      #linking cell centers to unique_latlon
      unique_latlon[,cell_center_longitude_s := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]
    
        #link centers back to main data table
      reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude"))
    
      #number of tows in each cell
      towcount <- unique_latlon[, .N, by = cell]
    
      #get the grid cell boundary for cells which had trawls
      grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)
    
      #update grid properties to include # of trawls in each cell
      grid <- merge(grid, unique_latlon, by = "cell")
      
      #Any years where clearly fewer cells were sampled?
      year_cells <- reduced_FishGlob.10year.gridded[,.(cell_count = length(unique(cell))),year]
      
      #set year benchmark to 70%
      benchmark <- percent_thresholds[21] * max(year_cells[,cell_count]) # of cells/ year to cut off below
      
     # assign(paste0("benchmark_",percent_thresholds[j]*100,"%"), benchmark) #unhash if you want to save object
      
      #only keep years where over x% of cells are sampled
      year_cells[,benchmark_met := cell_count > benchmark]
      
  #    years_deleted <- year_cells[benchmark_met == F]$year #which years are left out?
      
      years_kept <- year_cells[benchmark_met ==T]$year #which years  to keep
      
  #    years_deleted_percent <- length(years_deleted)/nrow(year_cells)*100
      
  #    years_deleted_count <- length(years_deleted)
      
  #    year_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], percent_thresholds[j],years_deleted_percent, years_deleted_count), nrow = 1))
      
      
   #   year_threshold_sensitivity_full <- rbind(year_threshold_sensitivity_full, year_threshold_sensitivity, use.names = F)
      
      #reduce to years that are well sampled
      reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
      
      #identify any cells that in any years are sampled less than 3 times
      reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
      
      for (k in 1:length(cell_count_thresholds)) {
      #cell ids to remove and keep
      #in any year, which cells are sampled less than x times, these need to go
      cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < cell_count_thresholds[k],cell]) 
      
      cells_deleted_percent <- length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))
      #what percent of cells are deleted
      
      cells_deleted_count <- length(cell_id_remove)
      
      #reduce to cells that are well sampled
      reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!cell %in% cell_id_remove,]
      
      #add to cleaned data table of all regions
      FishGlob_cleaned_cell_sensitivity  <- rbind(FishGlob_cleaned_cell_sensitivity, reduced_FishGlob.10year.gridded.r.cell)
      
      #What percent of observations does this remove?
      obs_removed_percent <- (nrow(reduced_FishGlob.10year.gridded.r)-nrow(reduced_FishGlob.10year.gridded.r.cell))/nrow(reduced_FishGlob.10year.gridded.r) #what % obs do we lose
      
            obs_removed_count <- nrow(reduced_FishGlob.10year.gridded.r)-nrow(reduced_FishGlob.10year.gridded.r.cell) #what # obs do we lose
      
      #add to row in small data.table
           cell_threshold_sensitivity <- data.table(matrix(c(all_survey_seasons[i], cell_count_thresholds[k],cells_deleted_percent, cells_deleted_count, obs_removed_percent, obs_removed_count), nrow = 1))
      
      #combine with full data.table
      cell_threshold_sensitivity_full <- rbind(cell_threshold_sensitivity_full, cell_threshold_sensitivity, use.names = F)
      
  
      }
     print(paste0(all_survey_seasons[i]))
    }

#delete first empty row
cell_threshold_sensitivity_full <- cell_threshold_sensitivity_full[-1,]
cell_threshold_sensitivity_full[,cell_count_threshold := as.numeric(cell_count_threshold)][,cells_deleted_percent := as.numeric(cells_deleted_percent)][,cells_deleted_count := as.numeric(cells_deleted_count)][,obs_removed_percent := as.numeric(obs_removed_percent)][,obs_removed_count := as.numeric(obs_removed_count)]

#Make plot
cell_threshold_sensitivity_plot <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = cells_deleted_percent)) +
  geom_point()
  geom_line(aes(color = survey_season)) +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
cell_threshold_sensitivity_plot_facet <- ggplot(cell_threshold_sensitivity_full, aes(x = percent_threshold, y = cells_deleted_percent)) +
  geom_line() +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
cell_threshold_sensitivity_plot_box <- ggplot(year_threshold_sensitivity_full, aes(y = cells_deleted_percent, x = percent_threshold, group = as.factor(percent_threshold))) +
  geom_boxplot() +
  theme_classic()

ggsave(cell_threshold_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

cell_threshold_sensitivity_merge <- plot_grid(cell_threshold_sensitivity_plot + theme(legend.position = "none"), cell_threshold_sensitivity_plot_box + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(cell_threshold_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "cell_threshold_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")


#What about observations instead of cells?
#Make plot
cell_threshold_obs_sensitivity_plot <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = 1-obs_removed_percent)) +
  geom_line(aes(color = survey_season)) +
      labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
#  facet_wrap(~survey_season) +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot.jpg", width = 10, height = 5, unit = "in")

#Make faceted plot
cell_threshold_obs_sensitivity_plot_facet <- ggplot(cell_threshold_sensitivity_full, aes(x = cell_count_threshold, y = 1-obs_removed_percent)) +
  geom_line() +
        labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
  facet_wrap(~survey_season, ncol = 3) +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot_facet, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot_facet.jpg", unit = "in", width = 3, height = 15)

#Make box plot
cell_threshold_obs_sensitivity_plot_box <- ggplot(cell_threshold_sensitivity_full, aes(y = 1-obs_removed_percent, x = cell_count_threshold, group = cell_count_threshold)) +
        labs(x = "Tows per Cell Threshold", y = "Percent Hauls Maintained") +
  geom_boxplot() +
  theme_classic()

ggsave(cell_threshold_obs_sensitivity_plot_box, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_plot_box.jpg", width = 10, height = 5, unit = "in")

cell_threshold_obs_sensitivity_merge <- plot_grid(cell_threshold_obs_sensitivity_plot + theme(legend.position = "none"), cell_threshold_obs_sensitivity_plot_box  + theme(axis.title.y = element_blank(), axis.text.y = element_blank()), ncol = 2, align = "hv")

ggsave(cell_threshold_obs_sensitivity_merge, path = here::here("figures","sensitivity"),filename = "cell_threshold_obs_sensitivity_merge.jpg", width = 6, height = 3, unit = "in")
```
