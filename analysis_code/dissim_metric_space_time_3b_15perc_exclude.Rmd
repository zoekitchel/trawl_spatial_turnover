---
title: "Calculating Dissimilarity Metrics Across Space and Time for top 85% of species"
output: html_notebook
date: May 9, 2024
author: Zoe Kitchel
---
Script 3b for Kitchel et al. 2024 in prep taxonomic diversity manuscript.

```{r setup}
library(data.table)
library(vegan)
library(sf)
library(concaveman) #polygon around points
library(betapart) #allows us to partition beta diversity
library(geosphere)
library(ggpubr) #stat_regline_equation
library(nlme)


FishGlob_clean.singleseason <- readRDS(here::here("data", "cleaned", "FishGlob_clean.singleseason.rds"))


```

Loop through all regions (takes  a few hours to run)

```{r list of all survey season combinations}
all_survey_units <- unique(FishGlob_clean.singleseason[, survey_unit])
```

Delete 15% of least abundant species by abundance or biomass (across whole time series)
```{r}
#identify whether you will use biomass or abundance to identify least abundant species

#abundance subset (those with abundance data)
abun_subset <- c(
"BITS-1",
"EVHOE",
"FR-CGFS",
"IE-IGFS",
"MEDITS",
"NIGFS-1",
"NS-IBTS-3",
"PT-IBTS",
"ROCKALL",
"SWC-IBTS-1",
"AI",
"DFO-NF",
"EBS",
"GMEX-Fall",
"GOA",
"GRL-DE",
"GSL-N",
"GSL-S",
"ICE-GFS",
"NEUS-Fall",
"Nor-BTS-3",
"SCS-SUMMER",
"SEUS-fall",
"WCANN"
)

#add new columns that sum either abundance or biomass for each individual species per survey
#Define function to correctly sum across duplicates (sum(NA,NA,NA) = NA, while sum(1,NA,NA) = 1, which is not the default for na.rm parameter)

my_sum <- function(x){
  if(all(is.na(x))){
    return(NA)
  }
  else{
    return(sum(x, na.rm = TRUE))
  }
}

#sum total # or total biomass depending on whether abundance data or biomass data are available (prioritize abundance, but use biomass if no abundance available)
FishGlob_clean.singleseason[survey_unit %in% abun_subset,abundance_spp_sum := my_sum(num_cpue),.(accepted_name, survey_unit)][survey_unit %in% abun_subset,abundance_sum := sum(num_cpue),survey_unit][!(survey_unit %in% abun_subset),abundance_spp_sum := my_sum(wgt_cpue),.(accepted_name, survey_unit)][!(survey_unit %in% abun_subset),abundance_sum := sum(wgt_cpue),survey_unit]



FishGlob_abundance_survey <- unique(FishGlob_clean.singleseason[,.(survey_unit, accepted_name, abundance_spp_sum, abundance_sum)])

FishGlob_abundance_survey[,perc_spp_total_abundance := abundance_spp_sum/abundance_sum][,spp_count := uniqueN(accepted_name),survey_unit][,spp_count15_percent := round(0.15*spp_count,0)]

#rank 15% least abundant spp 
#rank by abundance
setkey(FishGlob_abundance_survey, perc_spp_total_abundance)

FishGlob_abundance_survey[, rank_abundance_bysurvey := frank(perc_spp_total_abundance), survey_unit]

#code each spp as 85% most common by abundance or 15% least common by abundance in a survey
FishGlob_abundance_survey[, exclude_spp := ifelse(rank_abundance_bysurvey <= spp_count15_percent, T,F)]

#spp and survey key
spp_survey_keep_key <- unique(FishGlob_abundance_survey[,.(survey_unit, accepted_name, exclude_spp)])

FishGlob_clean.singleseason_15perc_excluded <- spp_survey_keep_key[FishGlob_clean.singleseason, on = .(survey_unit, accepted_name)]

#drop all rows where speceis should be dropped
FishGlob_clean.singleseason_15perc_excluded <- FishGlob_clean.singleseason_15perc_excluded[exclude_spp == F,]

#drops 
nrow(FishGlob_clean.singleseason)-nrow(FishGlob_clean.singleseason_15perc_excluded)
#1811/2220730 rows (0.08% of observations)

saveRDS(FishGlob_clean.singleseason_15perc_excluded, here::here("output","FishGlob_clean.singleseason_15perc_excluded.Rds"))
```

###Dataset statistics

Delete any tows without any observations
Add new column presence = 1 where if either abundance or biomass >1, presence = YES
```{r}
#Calculate total number of critters and total biomass of critters in each tow. In this case, it's okay if sum(NA,NA,NA) --> 0, because we will still delete hauls with all 0s

FishGlob_clean.singleseason_15perc_excluded[,tow_abundance_sum := sum(num_cpue, na.rm = T),.(haul_id)]
FishGlob_clean.singleseason_15perc_excluded[,tow_biomass_sum := sum(wgt_cpue, na.rm = T),.(haul_id)]
hist(FishGlob_clean.singleseason_15perc_excluded$num_cpue)
hist(FishGlob_clean.singleseason_15perc_excluded$wgt_cpue)

summary(FishGlob_clean.singleseason_15perc_excluded$tow_abundance_sum)
summary(FishGlob_clean.singleseason_15perc_excluded$tow_biomass_sum)

tows_by_abundance <- unique(FishGlob_clean.singleseason_15perc_excluded[,.(haul_id, tow_abundance_sum)])
tows_by_biomass <- unique(FishGlob_clean.singleseason_15perc_excluded[,.(haul_id, tow_biomass_sum)])
tows_noabundancebiomass <- unique(FishGlob_clean.singleseason_15perc_excluded[!(tow_abundance_sum > 0 | tow_biomass_sum > 0),.(haul_id, tow_abundance_sum,tow_biomass_sum, survey_unit, year)]) #there are no rows where BOTH abundance and biomass are 0 or NA

#New presence absence column
FishGlob_clean.singleseason_15perc_excluded[,Present := ifelse(num_cpue>0|wgt_cpue>0,1,0)]

#Delete absenses (which are NA's) Drops from 2224344 to 2219440, 
FishGlob_clean.singleseason_15perc_excluded <- FishGlob_clean.singleseason_15perc_excluded[!is.na(Present),] 

#From  above, this new column should be 100% 1
summary(FishGlob_clean.singleseason_15perc_excluded$Present) #check yes

length(unique(FishGlob_clean.singleseason_15perc_excluded[tow_abundance_sum <= 0,haul_id])) #13123 tows out of 178531 have no abundance observations (o for all), 7%
length(unique(FishGlob_clean.singleseason_15perc_excluded[tow_biomass_sum <= 0,haul_id])) #25074 tows out of 178531 have no abundance observations (o for all), 14%
#KEEP IN MIND ALL BIOMASS OBSERVATIONS FOR EUROPE SHOULD BE DELETED ANYWAY BECAUSE THEY WERE CALCULATED USING LENGTH WEIGHT RELATIONSHIPS AND WERE NOT MEASURED CONSISTENTLY ON BOARD

#clean up
rm(tows_by_abundance, tows_by_biomass)

```

Total observations? (spp x tows)
```{r}
nrow(FishGlob_clean.singleseason_15perc_excluded)
```

Total tows?
```{r}
length(unique(FishGlob_clean.singleseason_15perc_excluded[,haul_id]))
```

Total survey units?
```{r}
length(unique(FishGlob_clean.singleseason_15perc_excluded[,survey_unit]))
```

Total species?
```{r}
length(unique(FishGlob_clean.singleseason_15perc_excluded[,accepted_name]))
```

##Calculate Beta Dissimilarity between all sampling sites within every survey unit

##Jaccard abundance calculations and regular Jaccard using VEGAN package 
Calculate dissimilarity using Jaccard for both abundance and biomass
```{r loop through all regions and years for jaccard indices}

jaccard_index_allyears <- data.table("survey" = character(), 
                                                 "survey_unit" = character(),
                                                "year" = integer(),
                                                "dissimilarity_metric" = character(),
                                                "annual_dissimilarity_value" = numeric())

for (i in 1:length(all_survey_units)) {
  
  FishGlob_clean.singleseason_subset <- FishGlob_clean.singleseason_15perc_excluded[survey_unit == all_survey_units[i], ]
  
  jaccard_index_onereg <- data.table("survey" = character(), 
                                                 "survey_unit" = character(),
                                                "year" = integer(),
                                                "dissimilarity_metric" = character(),
                                                "annual_dissimilarity_value" = numeric())
  
   #list years
  FishGlob_clean.singleseason_subset[, year:= as.numeric(year)] #make numeric
  setorder(FishGlob_clean.singleseason_subset,  year)
  years <- unique(FishGlob_clean.singleseason_subset[, year])
  
  #haul id keys
  haul_ids <- unique(FishGlob_clean.singleseason_subset[, haul_id])
  haul_ids_key <- data.table(haul_id = haul_ids,  key_ID = seq(1, length(haul_ids),  by = 1))
  
  
  #convert haul_ids to numeric key_ids
  FishGlob_clean.singleseason_subset <- FishGlob_clean.singleseason_subset[haul_ids_key,  on = "haul_id"]

          for (j in 1:length(years)) {
            reduced_year <- FishGlob_clean.singleseason_subset[year == years[j], ]
            
            #make sure that haul_id, survey_unit, latitude, longitude, key_id, wgt_cpue, num_cpue, year, month, accepted name and day are unique (this is an issue with Namibia, where there is somehow repetition for just 5 observations of one species (Yarrella blackfordi, fixed by this step))
            reduced_year.u <- unique(reduced_year[,.(haul_id, survey_unit, latitude, longitude, key_ID, wgt_cpue, num_cpue, Present, year, month, day, accepted_name)])
            
            #distances among cells
            setorder(reduced_year.u,  key_ID)
            
            latitude_longitude_haul_id <- unique(reduced_year.u[, .(latitude, longitude, key_ID)])
            distances <- distm(latitude_longitude_haul_id[, .(longitude, latitude)])
            key_IDs <- latitude_longitude_haul_id[, key_ID]
          
            colnames(distances) <- rownames(distances) <- key_IDs
          
            #wide to long
            haul_id_distances.l <- reshape2::melt(distances, varnames = (c("haul_id1",  "haul_id2")),  value.name = "distance")
            
            #make into data table
            haul_id_distances.l <- data.table(haul_id_distances.l)
            
########################################################################
########################################################################
  #First, BINARY Jaccard based on presence absence for all regions
            
             #Only include rows where presence = 1
            reduced_year_occurence <- copy(reduced_year.u[Present == 1, ])
            
              reduced_year_wide <- dcast(reduced_year_occurence,  key_ID + year ~ accepted_name,  value.var = "Present",  fun.aggregate = sum) #longitude to wide data for community matrix,  column names are cell then species
              
              
              ncols <- ncol(reduced_year_wide)
              communitymatrix.occurence <- reduced_year_wide[, 3:ncols] #community matrix
            
              #list of haul_id keys
              key_IDs_subset <- reduced_year_wide[,key_ID]
            
              #Calculate Jaccard index values
              Jaccard_index_binary <- vegdist(communitymatrix.occurence,  method = "jaccard", binary = TRUE) #Jaccard index
            
             
              #make into matrix
              Jaccard_index_binary.m <- as.matrix(Jaccard_index_binary,  labels=TRUE) 
            
              
              colnames(Jaccard_index_binary.m) <- rownames(Jaccard_index_binary.m) <- key_IDs_subset
            
            
            
              #reshape dissimilarities
              Jaccard_index_binary.l <- reshape2::melt(Jaccard_index_binary.m,  varnames = c("haul_id1",  "haul_id2"),  value.name = "jaccard_dissimilarity_index_binary")
            
              
              #and then to data table format
              Jaccard_index_binary.l <- data.table(Jaccard_index_binary.l)
              
              Jaccard_index_binary.l[,dissimilarity_metric := "jaccard_dissimilarity_index_binary"]
       
              #again, wide to long
               Jaccard_index_binary.long <- reshape2::melt(Jaccard_index_binary.l,
              id.vars = c("haul_id1",  "haul_id2"), variable.name = "dissimilarity_metric", value.name = "value", measure.vars = c("jaccard_dissimilarity_index_binary"))
            
              #merge distance with dissimilarity for this year with both metrics of dissimilarity
              dissimilarities_full <- haul_id_distances.l[Jaccard_index_binary.long,  on = c("haul_id1",  "haul_id2")]
              
              
              #delete if haul_id2 < haul_id1 (deletes comparisons between same site (0 inflated) and any duplicate comparisons)
              dissimilarities_full <- dissimilarities_full[haul_id2 < haul_id1]
            
              #take averages
              #yearly means, deleted a few rogue NAs for balanced changes and abundance gradients
                dissimilarities_full[,annual_dissimilarity_value := mean(value),.(dissimilarity_metric)]

                  #unique rows
                  dissimilarities_full.r <- unique(dissimilarities_full[,.(dissimilarity_metric, annual_dissimilarity_value)])

              #add survey survey unit
              dissimilarities_full.r[, "survey" := FishGlob_clean.singleseason_subset[1, survey]]
              dissimilarities_full.r[, "survey_unit" := all_survey_units[i]]
              #add year for these values
              dissimilarities_full.r[,  "year" := years[j]]
              
              #reorder
              dissimilarities_full.r <-  dissimilarities_full.r[,.(survey, survey_unit, year, dissimilarity_metric, annual_dissimilarity_value)]
   
              #remove full dataset
              rm(dissimilarities_full)
              rm(Jaccard_index_binary, Jaccard_index_binary.m, Jaccard_index_binary.long, Jaccard_index_binary.l)
              
              #bind with full for this reg
              jaccard_index_onereg <- rbind(jaccard_index_onereg, dissimilarities_full.r)

              print(paste0(j, "/", length(years))) #print year that we're on
  
         } #closes year
  
  #save the output from this region
  filename <- paste0(all_survey_units[i],"_jaccard_index.rds")
  saveRDS(jaccard_index_onereg, here::here("output","jaccard_index_VEGAN","by_region_15perc_excluded",filename))
  rm(jaccard_index_onereg) #attempt to save some memory here
  
  print(paste0("We have cycled through ",  i, " of ", length(all_survey_units),  " surveys"))
  
  } #closes survey/region

#merge all years into distances_dissimilarities_allyears
files <- list.files(here::here("output","jaccard_index_VEGAN","by_region_15perc_excluded"))

for(i in 1:length(files)){
  jaccard_index_byreg <- readRDS(here::here("output","jaccard_index_VEGAN","by_region_15perc_excluded",files[i]))
  jaccard_index_allyears <- rbind(jaccard_index_allyears, jaccard_index_byreg)
}


#Be sure to check which one of these you actually want to un-hash!

saveRDS(jaccard_index_allyears,  here::here("output","jaccard_index_VEGAN", "jaccard_index_allyears_15perc_excluded.rds"))

#jaccard_index_allyears <- readRDS(here::here("output","jaccard_index_VEGAN", "jaccard_index_allyears_15perc_excluded.rds"))
```

