---
title: "Distance Decay All Surveys"
output: html_notebook
---



```{r setup}
library(data.table)
library(vegan)
library(sf)
library(concaveman) #polygon around points
library(betapart) #allows us to partition beta diversity
library(geosphere)
library(ggpubr) #stat_regline_equation

#unhash typically
#FishGlob_cleaned.10year<- readRDS(here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.rds"))


```

Set up to look at distance decay for all survey x season combinations
```{r}
all_season_surveys <- unique(FishGlob_cleaned.10year[,survey_season])
```

Check to see if any are missing CPUE?
```{r missing cpue}
View(FishGlob_cleaned.10year[,sum(is.na(wgt_cpue)),survey_season])
```

Survey Season: NAs for wgt_cpue
MEDITS_NA: 291896 (no weights, only abundances, use abundances (num_cpue) instead?)
NEUS_Spring: 100287 (will first look at tow duration, delete any that look wonky, and then divide wgt by 30 minutes)
NEUS_Fall: 92488 (will first look at tow duration, delete any that look wonky, and then divide wgt by 30 minutes)
IS-MOAG_NA: 12014 (no weights, only abundances, use abundances (num_cpue) instead?)
GSL-N_NA: 3899 (we don't have area swept for these observations 3899/55422 --> 7%, not a big deal, just leave out)
FR-CGFS_NA: 2045 (we don't have weight for these observations, 2045/20380 --> 10%, not a big deal, just leave out)
BITS_NA: 1149 (we don't have weight for these observations, 1149/63167 --> 1.8%, not a big deal, just leave out)
NS-IBTS_NA: 912 (we don't have weight for these observations, 912/107677 --> 0.8%, not a big deal, just leave out)
PT-IBTS_NA: 458 (we don't have weight for these observations, 458/11160 --> 4%, not a big deal, just leave out)
WCANN_NA: 178 (we don't have weight for these observations, 178/149905 --> 0.1%, not a big deal, just leave out)
NIGFS_NA: 81 (we don't have weight for these observations, 81/25638 --> 0.3%, not a big deal, just leave out)
EVHOE_NA: 35 (we don't have weight for these observations, 35/15828 --> 0.2%, not a big deal, just leave out)
IE-IGFS_NA: 30 (we don't have weight for these observations, 30/39822 --> 0.08%, not a big deal, just leave out)
SWC-IBTS_NA: 20 (we don't have weight for these observations, 20/7095 --> 0.2%, not a big deal, just leave out)

Remove  bad hauls (same code as marine_heatwave_trawl prep_trawl_data.R code from December 21, 2021)
```{r copied code from marine heatwaves}
#for northeast, we are going to delete any hauls before 2009 that are outside of +/- 5 minutes of 30 minutes and 2009 forward that are outside of +/- 5 minutes of 20 minutes
neus_bad_hauls <- unique(FishGlob_cleaned.10year[(survey == "NEUS" & year < 2009 & (haul_dur < 0.42 | haul_dur > 0.58)) | (survey == "NEUS" & year >= 2009 & (haul_dur < 0.25  | haul_dur > 0.42)),haul_id])
#this removes 122 hauls from 16989 total hauls (0.7%)

#calculate wgt_cpue (km^2 avg from sean Lucey) and wgt_h (all biomass values calibrated to standard pre 2009 30 minute tow)
FishGlob_cleaned.10year[survey == "NEUS", wgt_h := ifelse(is.na(wgt),NA, wgt/0.5)][
  survey == "NEUS", wgt_cpue := ifelse(is.na(wgt),NA, wgt/0.0384)][
    survey == "NEUS", num_h := ifelse(is.na(num),NA, num/0.5)][
      survey == "NEUS", num_cpue := ifelse(is.na(num),NA, num/0.0384)]

# get haul-level data
haul_info <- copy(FishGlob_cleaned.10year)[, .(survey, country, haul_id, year, month, latitude, longitude)] %>% unique() # lots of other useful data in here like depth, just trimming for speed 
bad_hauls <- (copy(haul_info)[, .N, by=.(haul_id)][ N > 1 ])$haul_id # find duplicated hauls
bad_hauls <- c(bad_hauls, "EVHOE 2019 4 FR 35HT GOV X0510 64") #add EVHOE long haul (24 hours; EVHOE 2019 4 FR 35HT GOV X0510 64) to bad hauls
GSLN_hauls_delete <- unique(FishGlob_cleaned.10year[survey == "GSL-N" & year < 1987,haul_id])#get rid of hauls before 1987 for GSL-N because there are only biomass data for 2 species in 1984, and then no biomass data for 2 years
bad_hauls <- c(bad_hauls, GSLN_hauls_delete, neus_bad_hauls)
haul_info <- haul_info[!haul_id %in% bad_hauls] # filter out bad hauls
length(unique(haul_info$haul_id))==nrow(haul_info) # check that every haul is listed exactly once 
FishGlob_cleaned.10year.r <- copy(FishGlob_cleaned.10year)[haul_id %in% haul_info$haul_id]

#This edit brings 2032538 down to 2029697 observations (Loss of 0.1% of observations)
rm(FishGlob_cleaned.10year)

#now, delete any rows without cpue data, shouldn't be too many (don't do this! want to look at abundances for IS-MOAG and MEDITS)

abundances_only <- c("IS-MOAG", "MEDITS")

#FishGlob_cleaned.10year_final <- copy(FishGlob_cleaned.10year.r)[!is.na(wgt_cpue),]

#This edit brings 2029697 down to 1716948 observations (Loss of 15% of observations)

#check
summary(FishGlob_cleaned.10year.r$wgt_cpue) #why inf? 13 observations from GSL-N, I will delete these
FishGlob_cleaned.10year.r <- copy(FishGlob_cleaned.10year.r)[!is.infinite(wgt_cpue),]

#for now, limit to years after 1981 (maybe use other dataset (SODA) for SCS?)
FishGlob_cleaned.10year.r <- FishGlob_cleaned.10year.r[year >= 1981,]

saveRDS(FishGlob_cleaned.10year.r, here::here("output","region_season_cleaned_data","FishGlob_cleaned.10year.r.rds"))
```


Loop through all regions
```{r loop through all regions and years}
survey_stats <- data.table(survey = character() , season = character(), survey_season = character(), spp_num = numeric(), study_period = numeric(), study_duration = numeric(), lat_range = numeric(), mid_lat = numeric(), lon_range = numeric(), area = numeric(), depth_range = numeric(),  mid_depth = numeric())

distances_dissimilarities_allyears <- data.table("haul_id1" = integer(), "haul_id2" = integer(), "distance" = numeric(),"bray_curtis_dissimilarity_balanced" = numeric(), survey = character(), season = character(), season_survey = character(), year = integer(), "jaccard_dissimilarity_turnover" = numeric(), abund_biomass  =  character())

for (i in 1:length(all_season_surveys)) {
  
  FishGlob_cleaned.10year_subset <- FishGlob_cleaned.10year.r[survey_season == all_season_surveys[i],]
  
  #maps
  ####unique lat lon
  lat_lon <- unique(FishGlob_cleaned.10year_subset[,.(latitude, longitude_s)])

  pts <- st_as_sf(lat_lon, coords=c('longitude_s','latitude'), crs=4326 )

  conc <- concaveman(pts, concavity = 5)
  sf::sf_use_s2(FALSE) #turn off the s2 processing; GEOS treats projected coordinates as planar (i.e. two points lie on a line of infinite max lenght) while s2 is more "correct" (two points lie on a great circle of circumference of 40 075 kilometers)
  area <- st_area(conc) #m2, check this later

  #fill row
 row <- data.table(FishGlob_cleaned.10year_subset[1,survey],
                        FishGlob_cleaned.10year_subset[1,season],
                        FishGlob_cleaned.10year_subset[1,as.character(survey_season)],
                        as.numeric(length(unique(FishGlob_cleaned.10year_subset[,accepted_name]))),
                        as.numeric(max(FishGlob_cleaned.10year_subset[,year])-min(FishGlob_cleaned.10year_subset[,year])),
                        as.numeric(length(unique(FishGlob_cleaned.10year_subset[,year]))),
                        as.numeric(max(FishGlob_cleaned.10year_subset[,latitude])-min(FishGlob_cleaned.10year_subset[,latitude])),
                        as.numeric(mean(FishGlob_cleaned.10year_subset[,latitude])),
                        as.numeric(max(FishGlob_cleaned.10year_subset[,longitude])-min(FishGlob_cleaned.10year_subset[,longitude])),
                        as.numeric(area),
                        as.numeric(max(FishGlob_cleaned.10year_subset[,as.numeric(depth)], na.rm = T)-min(FishGlob_cleaned.10year_subset[,as.numeric(depth)], na.rm = T)),
                   as.numeric(mean(FishGlob_cleaned.10year_subset[,as.numeric(depth)], na.rm = T)))
 
   survey_stats <- rbind(survey_stats, row, use.names = F)
   
   #list years
  FishGlob_cleaned.10year_subset[,year:= as.numeric(year)] #make numeric
  setorder(FishGlob_cleaned.10year_subset, year)
  years <- unique(FishGlob_cleaned.10year_subset[,year])
  
  #haul id keys
  haul_ids <- unique(FishGlob_cleaned.10year_subset[,haul_id])
  haul_ids_key <- data.table(haul_id = haul_ids, key_ID = seq(1,length(haul_ids), by = 1))
  
  
  #convert haul_ids to numeric key_ids
  FishGlob_cleaned.10year_subset <- FishGlob_cleaned.10year_subset[haul_ids_key, on = "haul_id"]

          for (j in 1:length(years)) {
            reduced_year <- FishGlob_cleaned.10year_subset[year == years[j],]
            
            #distances among cells
            setorder(reduced_year, key_ID)
            
            latitude_longitude_haul_id <- unique(reduced_year[,.(latitude,longitude,key_ID)])
            distances <- distm(latitude_longitude_haul_id[,.(longitude,latitude)])
            key_IDs <- latitude_longitude_haul_id[,key_ID]
          
            colnames(distances) <- rownames(distances) <- key_IDs
          
            #wide to longitude
            haul_id_distances.l <- reshape2::melt(distances,varnames = (c("haul_id1", "haul_id2")), value.name = "distance")
            
            #make into data table
            haul_id_distances.l <- data.table(haul_id_distances.l)
          
          if(!(reduced_year[1,survey] %in% abundances_only)) {
            
            #if some rows have wgt_cpue missing, get rid of these rows
            reduced_year <- reduced_year[complete.cases(reduced_year[,wgt_cpue]),]
            
              reduced_year_wide <- dcast(reduced_year, key_ID + year ~ accepted_name, value.var = "wgt_cpue", fun.aggregate = sum) #longitude to wide data for community matrix, column names are cell then species
              
              
              ncols <- ncol(reduced_year_wide)
              communitymatrix <- reduced_year_wide[,3:ncols] #community matrix with year and cell on far right
              communitymatrix.occurence <- communitymatrix
              communitymatrix.occurence[communitymatrix.occurence > 0] <- 1
            
              #list of haul_id keys
              key_IDs_subset <- reduced_year_wide$key_ID
            
              dissimilarities_abundance <- beta.pair.abund(communitymatrix, index.family = "bray") #dissimilarity 
              dissimilarities_occurrence <- beta.pair(communitymatrix.occurence, index.family = "jaccard") #dissimilarity
            
              #make into matrix
              dissimilarities_abundance.m <- as.matrix(dissimilarities_abundance$beta.bray.bal, labels=TRUE) #bal = balanced
              dissimilarities_occurrence.m <- as.matrix(dissimilarities_occurrence$beta.jtu, labels=TRUE) #jtu = turnover
              colnames(dissimilarities_abundance.m) <- rownames(dissimilarities_abundance.m) <- key_IDs_subset
              colnames(dissimilarities_occurrence.m) <- rownames(dissimilarities_occurrence.m) <- key_IDs_subset
            
              #reshape dissimilarities
              dissimilarities_abundance.l <- reshape2::melt(dissimilarities_abundance.m, varnames = c("haul_id1", "haul_id2"), value.name = "bray_curtis_dissimilarity_balanced")
              dissimilarities_occurrence.l <- reshape2::melt(dissimilarities_occurrence.m, varnames = c("haul_id1", "haul_id2"), value.name = "jaccard_dissimilarity_turnover")
              dissimilarities_abundance.l <- data.table(dissimilarities_abundance.l) #and then to data table
              dissimilarities_occurrence.l <- data.table(dissimilarities_occurrence.l)
            
              #add year for these values
              dissimilarities_abundance.l[, "year" := years[j]]
              dissimilarities_occurrence.l[, "year" := years[j]]
              
              #what type of metric?
              dissimilarities_abundance.l[,"abund_biomass" := "biomass"]
              dissimilarities_occurrence.l[,"abund_biomass" := "biomass"]
            
              #merge distance with dissimilarity for this year with both metrics of dissimilarity
              dissimilarities_full <- haul_id_distances.l[dissimilarities_abundance.l, on = c("haul_id1", "haul_id2")]
              dissimilarities_full <- dissimilarities_full[dissimilarities_occurrence.l, on = c("haul_id1", "haul_id2", "year", "abund_biomass")]
              
              #add survey and season
              dissimilarities_full[,"survey" := FishGlob_cleaned.10year_subset[1,survey]]
              dissimilarities_full[,"season" := FishGlob_cleaned.10year_subset[1,season]]
              dissimilarities_full[,"season_survey" := all_season_surveys[i]]
            
              #add to data table
              distances_dissimilarities_allyears <- rbind(distances_dissimilarities_allyears, dissimilarities_full)
              
              print(paste0(j,"/",length(years)))
            
          } else { #if we do using abundance instead
                          reduced_year_wide <- dcast(reduced_year, key_ID + year ~ accepted_name, value.var = "num_cpue", fun.aggregate = sum) #longitude to wide data for community matrix, column names are cell then species
              
              
              ncols <- ncol(reduced_year_wide)
              communitymatrix <- reduced_year_wide[,3:ncols] #community matrix with year and cell on far right
              communitymatrix.occurence <- communitymatrix
              communitymatrix.occurence[communitymatrix.occurence > 0] <- 1
            
              #list of haul_id keys
              key_IDs_subset <- reduced_year_wide$key_ID
            
              dissimilarities_abundance <- beta.pair.abund(communitymatrix, index.family = "bray") #dissimilarity 
              dissimilarities_occurrence <- beta.pair(communitymatrix.occurence, index.family = "jaccard") #dissimilarity
            
              #make into matrix
              dissimilarities_abundance.m <- as.matrix(dissimilarities_abundance$beta.bray.bal, labels=TRUE) #bal = balanced
              dissimilarities_occurrence.m <- as.matrix(dissimilarities_occurrence$beta.jtu, labels=TRUE) #jtu = turnover
              colnames(dissimilarities_abundance.m) <- rownames(dissimilarities_abundance.m) <- key_IDs_subset
              colnames(dissimilarities_occurrence.m) <- rownames(dissimilarities_occurrence.m) <- key_IDs_subset
            
              #reshape dissimilarities
              dissimilarities_abundance.l <- reshape2::melt(dissimilarities_abundance.m, varnames = c("haul_id1", "haul_id2"), value.name = "bray_curtis_dissimilarity_balanced")
              dissimilarities_occurrence.l <- reshape2::melt(dissimilarities_occurrence.m, varnames = c("haul_id1", "haul_id2"), value.name = "jaccard_dissimilarity_turnover")
              dissimilarities_abundance.l <- data.table(dissimilarities_abundance.l) #and then to data table
              dissimilarities_occurrence.l <- data.table(dissimilarities_occurrence.l)
            
              #add year for these values
              dissimilarities_abundance.l[, "year" := years[j]]
              dissimilarities_occurrence.l[, "year" := years[j]]
              
              #what type of metric?
              dissimilarities_abundance.l[,"abund_biomass" := "abundance"]
              dissimilarities_occurrence.l[,"abund_biomass" := "abundance"]
            
              #merge distance with dissimilarity for this year with both metrics of dissimilarity
              dissimilarities_full <- haul_id_distances.l[dissimilarities_abundance.l, on = c("haul_id1", "haul_id2")]
              dissimilarities_full <- dissimilarities_full[dissimilarities_occurrence.l, on = c("haul_id1", "haul_id2", "year", "abund_biomass")]
              
              #add survey and season
              dissimilarities_full[,"survey" := FishGlob_cleaned.10year_subset[1,survey]]
              dissimilarities_full[,"season" := FishGlob_cleaned.10year_subset[1,season]]
              dissimilarities_full[,"season_survey" := all_season_surveys[i]]
            
              #add to data table
              distances_dissimilarities_allyears <- rbind(distances_dissimilarities_allyears, dissimilarities_full)
              
              print(paste0(j,"/",length(years)))
          
              } #closes if else abundance versus biomass

  
         } #closes year
  
  print(paste0("We have cycled through ", i," of ",length(all_season_surveys), " total survey/season combos"))
  
  } #closes survey/region


saveRDS(distances_dissimilarities_allyears, here::here("output","distance_decay","distances_dissimilarities_allyears.rds"))
```

Box plot for each region
```{r box plot each region}
box_plots_jaccard <- list()
box_plots_bray <- list()

bray_model_outputs <- data.table()

bray_model_outputs <- as.data.table(matrix(nrow = length(all_season_surveys)))
bray_model_outputs[, survey:=as.character(V1)][, season:=as.character(V1)][, season_survey:=as.character(V1)][, bray_coef:=as.numeric(V1)][, bray_intercept := as.numeric(V1)][,bray_coef_pvalue := as.numeric(V1)][,bray_r_squared := as.numeric(V1)]
bray_model_outputs[, V1 := NULL]

jaccard_model_outputs <- as.data.table(matrix(nrow = length(all_season_surveys)))

jaccard_model_outputs <- as.data.table(matrix(nrow = length(all_season_surveys)))
jaccard_model_outputs[, survey:=as.character(V1)][, season:=as.character(V1)][, season_survey:=as.character(V1)][, jaccard_coef:=as.numeric(V1)][, jaccard_intercept := as.numeric(V1)][,jaccard_coef_pvalue := as.numeric(V1)][,jaccard_r_squared := as.numeric(V1)]
jaccard_model_outputs[, V1 := NULL]


distances_dissimilarities_allyears[,year_f := as.factor(year)]

#in case you didn't make it earlier
all_season_surveys <- unique(distances_dissimilarities_allyears[,season_survey])

for (i in 1:length(all_season_surveys)) {
      distances_dissimilarities_allyears_subset <- distances_dissimilarities_allyears[season_survey == all_season_surveys[i]]
      
    #jaccard similarity
#    box_plots_jaccard[[i]] <- ggplot(distances_dissimilarities_allyears_subset,aes(year_f, #jaccard_dissimilarity_turnover)) +
#      geom_boxplot(outlier.shape = NA, lwd = 0.2) +
#      labs(x="Year", y = "Jaccard Dissimilarity") +
#      geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1), lwd = 0.3) +
#      stat_regline_equation() +
#      theme_classic() +
#      theme(axis.text.x = element_text(angle = 90))
#    
#    #bray curtis similarity
#    box_plots_bray[[i]] <- ggplot(distances_dissimilarities_allyears_subset,aes(year_f,bray_curtis_dissimilari#ty_balanced)) +
#      geom_boxplot(outlier.shape = NA, lwd = 0.2) +
#      labs(x="Year", y = "Bray Curtis Dissimilarity") +
#      geom_smooth(method = "lm", se=FALSE, color="red", aes(group=1), lwd = 0.3) +
#      stat_regline_equation() +
#      theme_classic() +
#      theme(axis.text.x = element_text(angle = 90))
    #START HERE GET THESE TO ACTUALLY SAVE
    #corresponding models
    avg_jaccard_mod <- lm(data = distances_dissimilarities_allyears_subset, formula = jaccard_dissimilarity_turnover ~ as.numeric(year))
    
    jaccard_model_outputs[i, "survey"] <- distances_dissimilarities_allyears_subset[1, survey]
    jaccard_model_outputs[i, "season"] <- distances_dissimilarities_allyears_subset[1, season]
    jaccard_model_outputs[i, "season_survey"] <- distances_dissimilarities_allyears_subset[1, season_survey]
    
    #coef
    
    jaccard_model_outputs[i,"jaccard_coef"] <- avg_jaccard_mod$coefficients[[2]]
    
    #intercept
    jaccard_model_outputs[i,"jaccard_intercept"] <- avg_jaccard_mod$coefficients[[1]]
    
    #p-value
     jaccard_model_outputs[i,"jaccard_coef_pvalue"] <- summary(avg_jaccard_mod)$coefficients[2,4]  
    
    #R^2
    jaccard_model_outputs[i,"jaccard_r_squared"] <- summary(avg_jaccard_mod)$r.squared
    
    #bray
    
    avg_bray_mod <- lm(data = distances_dissimilarities_allyears_subset, formula = bray_curtis_dissimilarity_balanced ~ as.numeric(year))
    
    bray_model_outputs[i, "survey"] <- distances_dissimilarities_allyears_subset[1, survey]
    bray_model_outputs[i, "season"] <- distances_dissimilarities_allyears_subset[1, season]
    bray_model_outputs[i, "season_survey"] <- distances_dissimilarities_allyears_subset[1, season_survey]
    
    #coef
    
    bray_model_outputs[i,"bray_coef"] <- avg_bray_mod$coefficients[[2]]
    #intercept
    bray_model_outputs[i,"bray_intercept"] <- avg_bray_mod$coefficients[[1]]
    
    #p-value
    bray_model_outputs[i,"bray_coef_pvalue"] <- summary(avg_bray_mod)$coefficients[2,4]  
    
    #R^2
    bray_model_outputs[i,"bray_r_squared"] <- summary(avg_bray_mod)$r.squared
    


}



#save plots and models as objects
saveRDS(jaccard_model_outputs, here::here("output","distance_decay","jaccard_model_outputs.rds"))
saveRDS(bray_model_outputs, here::here("output","distance_decay","bray_model_outputs.rds"))

saveRDS(box_plots_jaccard, here::here("output","distance_decay","box_plots_jaccard.rds"))
saveRDS(box_plots_bray, here::here("output","distance_decay","box_plots_bray.rds"))

#save plots as images

for (i in 1:length(all_season_surveys)) {
  filename_jaccard <- paste0(all_season_surveys[i], "_jaccard_box_plot.jpg")
  ggsave(box_plots_jaccard[[i]], path = here::here("figures", "distance_decay"), filename = filename_jaccard)
  
  filename_bray <- paste0(all_season_surveys[i], "_bray_box_plot.jpg")
  ggsave(box_plots_bray[[i]], path = here::here("figures", "distance_decay"), filename = filename_bray)
}
```




Averages for each survey_season
```{r}
colnames(distances_dissimilarities_allyears)


distances_dissimilarities_allyears[,mean_jaccard := mean(jaccard_dissimilarity_turnover), .(survey, season, year)]
distances_dissimilarities_allyears[,mean_bray := mean(bray_curtis_dissimilarity_balanced), .(survey, season, year)]
```

