---
title: "Prepping all Fish Glob Data"
output: html_notebook
---


```{r setup}
library(dggridR)
library(data.table)
library(rgdal)
library(raster)
library(sp)
library(rnaturalearth)
library(rnaturalearthdata)
library(rgeos)
library(taxize) #standardizing names
library(geosphere)  #to calculate distance between lat lon of grid cells
library(googledrive)
library(here)
```


Pull in compiled and cleaned data from fishglob February 4, 2022

```{r pull in data from fishglob for Eastern Bering Sea from Google Drive}
drive_download(file = "FISHGLOB_v1.1_clean.csv",
               path = here::here("data","FISHGLOB_v1.1_clean.csv"),
               overwrite = T)

FishGlob <- fread(here::here("data","FISHGLOB_v1.1_clean.csv"))

file.remove(here::here("data","FISHGLOB_v1.1_clean.csv"))

#save
saveRDS(FishGlob, file = here::here("data", "FishGlob_1.1.clean.rds"))

#load if already saved
FishGlob <- readRDS(file = here::here("data", "FishGlob_1.1.clean.rds"))

```

Get rid of any survey x season combos not sampled for at least 10 years

```{r summary by survey region}
#new row for total number of years sampled
FishGlob[,years_sampled := length(unique(year)),.(survey,season)]

hist(FishGlob$years_sampled)

#remove observations for any regions x season combinations sampled less than 10 times and observations not resolved to species
FishGlob.10year <- FishGlob[years_sampled >= 10,][rank == "Species",]

rm(FishGlob)

```

Survey specific changes (These have not actually been implemented as of February 6 2022)

From Batt et al. 2017
"In the Eastern Bering Sea, sampling years prior to 1984 (data begin in 1982) were excluded from analysis due to large apparent increases in the number of species recorded in the first two years."

"In the Gulf of Mexico, we restricted our analysis to data from 1984 - 2000 (full range
1982-2014); if all years had been used, the number of sites sampled in at least 85% of years would drop from 39 to 13."

"In the Southeast U.S., data from 1989 (data begin in 1989) were excluded because several sites were not sampled in this year, and if this year had been used, the number of sites sampled in at least 85% of years would drop from 24 to 23 (with only 21 sites sampled in 1989)."

"In the Northeast U.S., we excluded data from years prior to 1982 (data begin in 1968). Years prior to 1979 were excluded because strata in the southern tip of the region (between approximately 34.5o N and 35o N) were not regularly sampled during this time. A site in the Gulf of Maine (-69.25o E 43.25o N) was not sampled consistently between 1979 and 1981, and including these years in the analysis would have prevented this site from being included in the analysis (which would have reduced total sites from 100 to 99)."

"In Newfoundland, years prior to 1996 (first year was 1992) were excluded because many sites were not sampled. If all years had been used (1992 onward), total number of sites sampled in at least 85% of years would have been decreased from 191 to 53; if data from 1995 onward had been included, number of sites would have been 179." 

```{r}

FishGlob.10year.r <- FishGlob.10year[!(survey == "EBS" & year < 1984),][!(survey == "SEUS" & year < 1990),][!(survey == "NEUS" & year < 1982),][!(survey == "DFO-NF" & year < 1996),]

rm(FishGlob.10year)

#add column with season and survey
FishGlob.10year[,survey_season := as.factor(paste0(survey,"_",season))]

all_survey_seasons <- levels(FishGlob.10year[,survey_season])
```


Loop to standardize observations for all regions

```{r standardize observations all regions}

#world map
world <- data.table(map_data('world'))
north_america <- world[long >= -190 & long <= -130 & lat >= 50 & lat <= 80]

#if positive, subtract 360
unique_ai_latlon[,longitude_s := ifelse(longitude > 0,(longitude-360),(longitude))]

# Fixed map
ai_plot <- ggplot() +
geom_polygon(data = north_america, 
                                 aes(x=long, y = lat, group = group), 
                                 fill = "white", 
                                 color="black") +
    coord_fixed(xlim = c(-190, -140),  ylim = c(50, 80), ratio = 1.3) +
  geom_point(data = unique_ai_latlon,
    aes(
      x = longitude_s,
      y = latitude),
    color = "red", size = 0.05
  ) +
  theme_bw()

ai_plot

#set up grid
dggs <- dgconstruct(res = 8, metric = T) #with res = 8, we will need at least 3 observations per year within 7,774.2 km^2 (roughly size of some NEUS strata)

for (i in 1:length(all_survey_seasons)) {
  
  #reduce to specific survey/season combination
  reduced_FishGlob.10year <- FishGlob.10year[survey_season == all_survey_seasons[i],]
  
  #pull out unique lat lons
  unique_latlon <- unique(reduced_FishGlob.10year[,.(latitude, longitude)])
  
  unique_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for this region/survey combo

  #find cell centers
  cellcenters <- dgSEQNUM_to_GEO(dggs, unique_latlon[,cell])

  #linking cell centers to unique_EBS_latlon
  unique_latlon[,cell_center_longitude := cellcenters$lon_deg][,cell_center_latitude:= cellcenters$lat_deg]

    #link centers back to main data table
  reduced_FishGlob.10year.gridded <- merge(reduced_FishGlob.10year, unique_latlon, by = c("latitude", "longitude"))

  #number of tows in each cell
  towcount <- unique_latlon[, .N, by = cell]

  #get the grid cell boundary for cells which had trawls
  grid <- dgcellstogrid(dggs, unique_latlon[,cell], frame = T, wrapcells = F)

  #update grid properties to include # of trawls in each cell
  grid <- merge(grid, unique_latlon, by = "cell")
  
  #Any years for AI where clearly fewer cells were sampled?
  year_cells <- reduced_FishGlob.10year.gridded[,.(cell_count = length(unique(cell))),year]
  
  benchmark_90 <- 0.9 * max(year_cells[,cell_count])
  
  #only keep years where over 90% of cells are sampled
  year_cells[,benchmark_90 := cell_count > benchmark_90]
  
  years_deleted <- year_cells[benchmark_90 == F]$year #which years are left out?
  
  years_kept <- year_cells[benchmark_90 ==T]$year #which years  to keep
  
  years_deleted_percent <- round(length(years_deleted)/nrow(year_cells)*100,1)
  
  #print  the years that are left out
  print(ifelse(length(years_deleted) == 0, paste0(all_survey_seasons[i], " Years left out = 0"), paste0(all_survey_seasons[i], " Years left out = ", years_deleted, ", ",years_deleted_percent, "% of Years Excluded"))) #need to figure out how to list all years removed
  
  #reduce to years that are well sampled
  reduced_FishGlob.10year.gridded.r <- reduced_FishGlob.10year.gridded[year %in% years_kept,]
  
  #identify any cells that in any years are sampled less than 3 times
  reduced_FishGlob.10year.gridded.r[,year_cell_count := length(unique(haul_id)),.(year,cell)]
  
  #cell ids to remove and keep
  #in any year, which cells are sampled less than 3 times, these need to go
  cell_id_remove <- unique(reduced_FishGlob.10year.gridded.r[year_cell_count < 3]$cell)
  cells_deleted_percent <- round(length(cell_id_remove)/length(unique(reduced_FishGlob.10year.gridded.r[,cell]))*100,1)
  #this removes 40% of cells from Aleutian islands, seems like too much, I will return to this
  
    #print portion of cells that are left out
  print(ifelse(length(cell_id_remove) == 0, paste0(all_survey_seasons[i], " Cells left out = 0"), paste0(all_survey_seasons[i], " Cells left out = ", cell_id_remove, ", ",cells_deleted_percent, "% Cells Excluded"))) #need to figure out how to list all cells removed
  
    #reduce to cells that are well sampled
  reduced_FishGlob.10year.gridded.r.cell <- reduced_FishGlob.10year.gridded.r[!cell %in% cell_id_remove,]

```



```{r pull in EBS}

# Get Unique lines in the data table
unique_EBS_latlon<- unique(ebs[,.(latitude, longitude)])
unique_EBS_latlon.sf <- unique_EBS_latlon
  
#make lat lon coordinates into polygon
coordinates(unique_EBS_latlon.sf) <- ~latitude + longitude
unique_EBS_latlon.sf@proj4string <- crs("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

saveRDS(unique_EBS_latlon, "unique_EBS_latlon.rds")

#plot north america
world <- ne_countries(scale = "medium", returnclass = "sp")

world_ext <- extent(-180, -150,50, 67)

north_america_spdf <- crop(world, world_ext)

north_america_df <- fortify(north_america_spdf)

(na_outline <- ggplot(north_america_df, aes(long, lat, group = group)) +
  geom_polygon(color = "white", fill = "grey") +
  labs(x = "Longitude", y = "Latitude") +
  coord_equal() +
  theme_classic() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 1)) +
 geom_point(unique_EBS_latlon, mapping = aes(longitude, latitude), inherit.aes = FALSE, shape = 18, color = "red") +
  coord_equal())

```

Playing around with dggridR (package that makes grid cells)
```{r learning dggridR}
dggs <- dgconstruct(spacing = 111, metric = T, resround = 'nearest')

unique_EBS_latlon[,cell := dgGEO_to_SEQNUM(dggs, longitude, latitude)] #get corresponding grid cells for EBS trawl

#centersof cells
EBS_cellcenters <- dgSEQNUM_to_GEO(dggs, unique_EBS_latlon[,cell])

#linking cell centers to unique_EBS_latlon
unique_EBS_latlon[,LON_CENTER := EBS_cellcenters$lon_deg][,LAT_CENTER := EBS_cellcenters$lat_deg]

#link centers back to main data table

ebs <- merge(ebs, unique_EBS_latlon, by = c("latitude", "longitude"))

#number of tows in each cell
towcount <- unique_EBS_latlon[, .N, by = cell]

#get the grid cell boundary for cells which had trawls
grid_EBS <- dgcellstogrid(dggs, unique_EBS_latlon[,cell], frame = T, wrapcells = F)

#update grid properties to include # of trawls in each cell
grid_EBS <- merge(grid_EBS, unique_EBS_latlon, by = "cell")

ggplot(data = grid_EBS, aes(x = long, y = lat, group = cell)) +
  geom_polygon() +
  coord_quickmap()

saveRDS(grid_EBS, "grid_EBS.rds")

```

Trying to plot with grid overlaying
```{r grid overlaying}

na_outline + 
   geom_polygon(grid_EBS, mapping = aes(x = long,y = lat, group = cell), inherit.aes = FALSE, color = "black", fill = NA, size = 0.1)

ggsave(filename = "grid_EBS_cells_spacing_111.pdf")
  
```

Now, we'll overlay grid cells onto this list of tows (ebs) using lat and lon and gridrr package

```{r get rid of underused grid cells}

year_grid <- data.table(table(ebs$cell, ebs$year))

#there are some grid cells with infrequent records, I will delete these
grid_cells_to_exclude <- unique(year_grid[N == 0]$V1)

#delete these grid cells from full data table
ebs.reduced <- ebs[!cell %in% grid_cells_to_exclude]

#270643 observations left, this should be great

#how many tows (TOW) per cell in a given year

haul_cell_unique <- unique(ebs.reduced[,.(haul_id, cell, year)])

ebs_samplingevents_byyear <- haul_cell_unique[,.N, .(year, cell)]

ebs_cells_lessthan3 <- ebs_samplingevents_byyear[N<3,]

cells_to_delete <- unique(ebs_cells_lessthan3[,cell])

ebs.reduced.again <- ebs.reduced[!cell %in% cells_to_delete]

#down to 114793 observations

saveRDS(ebs.reduced.again, "ebs_data_gridded.rds")

```

Delete cells that aren't sampled regularly (missing more than 2 years from 1972-2019):

32234: sampled sporadically
all others have great coverage

Infrequently sampled cells
```{r infrequently sampled cells}

dump <- 32234


#delete these grid cells from full data table
grid_EBS.reduced.again.again <- ebs.reduced.again[cell != dump]


#114262 observations (no change)

```

This is where I would use rarefaction or coverage, for now I will skip
```{r rarefaction or coverage?}
#how many tows (TOW) per cell in a given year

cell_tow_year <- unique(grid_EBS.reduced.again.again[,.(haul_id, cell, year)])

ebs_samplingevents_byyear <- cell_tow_year[,.N, .(year, cell)]

ebs_cells_lessthan3 <- ebs_samplingevents_byyear[N<3,] #need three to build rarefaction curve, so, will get rid of less than three observations

cells_to_delete <- unique(ebs_cells_lessthan3[,cell])

ebs_full_1982onward.clean.reduced.again <- grid_EBS.reduced.again.again[!cell %in% cells_to_delete]

#1299247 observations

saveRDS(ebs_full_1982onward.clean.reduced.again, "ebs_data_gridded.rds")

```


Find distance between center of each grid cell
```{r distance between grid cells}

#just need grid cell # and center lat lon for each grid cell 
grid_EBS_cells <- unique(grid_EBS.reduced.again.again[,.(cell, LON_CENTER, LAT_CENTER)])

grid_EBS_cells <- setorder(grid_EBS_cells, cell)

ebs_reg_distances <- distm(grid_EBS_cells[,.(LON_CENTER, LAT_CENTER)])

colnames(ebs_reg_distances) <- grid_EBS_cells$cell
rownames(ebs_reg_distances) <- grid_EBS_cells$cell

#reorient to long form 
ebs_reg_distances.l <- reshape2::melt(as.matrix(ebs_reg_distances), varnames = c("cell1", "cell2"), value.name = "distance(m)") #matrix to data frame
ebs_reg_distances.l <- data.table(ebs_reg_distances.l) #and then to data table
ebs_reg_distances.l <- ebs_reg_distances.l[cell1 > cell2,] # get rid of repetitions and self-comparisons

```

Check the # of tows per grid cell/year combo
```{r number of tows per grid cell/year/region combo}
#cell/year/region combos
ebs_unique_tows_cell <- grid_EBS.reduced.again.again[,.SD[1],.(year, cell, haul_id)]

#no years have way more tows than others
plot(ebs_unique_tows_cell[,.N,.(year,cell)]$year, ebs_unique_tows_cell[,.N,.(year,cell)]$N)

#therefore, I will not leave out any years, but rather just pull out cells in years they are undersampled (<N=3, which is )

summary(ebs_unique_tows_cell[,.N,.(year,cell)]$N < 3) #this occurs in 538/2081 cell/year combos  (26%) (THIS IS A LOT, return to later)

#I don't think I need to exclude in ALL years, just in a few years
#an alternative would be to assess coverage using biomass (# of singletons and doubletons)

```

Delete all observations with less than 3 tows in a given year/cell
```{r delete observations with less than 3 tows}
ebs_counts <- ebs_unique_tows_cell[,.N,.(year,cell)]
ebs_keep <- ebs_counts[N>2,][,N := NULL]

grid_EBS.reduced_3plustows <- grid_EBS.reduced.again.again[ebs_keep, on = .(cell, year)]


```

Summary of area sampled
```{r sum area sampled}
#distance between furthest north and furthest south point
library(geosphere)
max_north_south_km <- (distm(c(mean(grid_EBS.reduced_3plustows$longitude), min(grid_EBS.reduced_3plustows$latitude)), c(mean(grid_EBS.reduced_3plustows$longitude), max(grid_EBS.reduced_3plustows$latitude)), fun = distHaversine))/1000
#distance between furthest east and furthest west point
max_west_east_km <- (distm(c(min(grid_EBS.reduced_3plustows$longitude), mean(grid_EBS.reduced_3plustows$latitude)), c(max(grid_EBS.reduced_3plustows$longitude), mean(grid_EBS.reduced_3plustows$latitude)), fun = distHaversine))/1000

max_north_south_km
max_west_east_km

summary(grid_EBS.reduced_3plustows$depth)

length(unique(grid_EBS.reduced_3plustows$accepted_name))

sort(unique(as.numeric(ebs$year)))
length(unique(ebs$year))
```

Make a new map but with only points we actually use
```{r new map points we use}
(na_outline_2 <- ggplot(north_america_df, aes(long, lat, group = group)) +
  geom_polygon(color = "white", fill = "grey") +
  labs(x = "Longitude", y = "Latitude") +
  coord_equal() +
  theme_classic() +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.border = element_rect(color = "black", fill = NA, size = 1)) +
 geom_point(unique(grid_EBS.reduced_3plustows[,.(latitude, longitude)]), mapping = aes(longitude, latitude), inherit.aes = FALSE, shape = 18, size = 0.001, color = "red") +
  coord_equal())
```

Save
```{r save}
#saveRDS(grid_EBS.reduced, "/Users/zoekitchel/Documents/grad school/Rutgers/Repositories/trawl_spatial_turnover/data/gridded/USA-EBS/grid_EBS.reduced.rds")


saveRDS(ebs_reg_distances.l, "/Users/zoekitchel/Documents/grad school/Rutgers/Repositories/trawl_spatial_turnover/data/gridded/USA-EBS/ebs_reg_distances.l.rds")

saveRDS(grid_EBS.reduced_3plustows,"/Users/zoekitchel/Documents/grad school/Rutgers/Repositories/trawl_spatial_turnover/data/gridded/USA-EBS/grid_EBS.reduced_3plustows.rds")


```

