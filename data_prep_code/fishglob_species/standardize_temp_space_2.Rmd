---
title: "Apply Temporal and Spatial Standardization (70/70)"
author: ZoÃ« J. Kitchel
date: October 11, 2023
---

Script 2 for Kitchel et al. 2023 in prep taxonomic diversity manuscript.

```{r setup}
library(tidyverse)
library(data.table)
library(here)
library(sp)
library(raster)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
library(concaveman)
library(sf)
library(viridis)
set.seed(1)
```

Load manually reduced fishglob database from prepare_fishglob_dataset.Rmd

*Users cannot access this data because it includes private bottom trawl data. However, you can create your own intermediate file with the publicly available FISHGLOB dataset.*

```{r}
FishGlob.10year.spp_manualclean <- readRDS(here::here("data","cleaned","FishGlob.10year.spp_manualclean.rds"))

#survey names
sort(unique(FishGlob.10year.spp_manualclean[,survey_unit]))
```

Survey name helper
```{r}
name_helper <- data.table(Survey_Name_Season = c("Aleutian Islands",
                                    "Baltic Sea Q1",
                                    "Baltic Sea Q4",
                                    "Chile",
                                    "Newfoundland",
                                    "Queen Charlotte Sound",
                                    "Eastern Bering Sea",
                                    "Bay of Biscay",
                                    "English Channel",
                                    "Gulf of Mexico Summer",
                                    "Gulf of Alaska",
                                    "Greenland",
                                    "N Gulf of St. Lawrence",
                                    "S Gulf of St. Lawrence",
                                    "Iceland",
                                    "Irish Sea",
                                    "Mediterranean",
                                    "Namibia",
                                    "NE US Fall",
                                    "NE US Spring",
                                    "N Ireland Q1",
                                    "N Ireland Q4",
                                    "Barents Sea Norway Q3",
                                    "N Sea Q1",
                                    "N Sea Q3",
                                    "Chatham Rise NZ",
                                    "E Coast S Island NZ",
                                    "W Coast S Island NZ",
                                    "Portugal",
                                    "S Georgia",
                                  "Scotian Shelf Summer",
                                  "SE US Fall",
                                  "SE US Spring",
                                  "SE US Summer",
                                  "W Coast US",
                                  "Atlantic Ocean ZA",
                                  "Indian Ocean ZA",
                                   "Rockall Plateau",
                                  "Scotland Shelf Sea Q1",
                                  "Scotland Shelf Sea Q4",
                                  "Falkland Islands",
                                  "Gulf of Mexico Fall",
                                  "Sub-Antarctic NZ",
                                  "Scotian Shelf Spring"),
                          survey_unit = c(
                                  "AI",        
                                  "BITS-1",    
                                  "BITS-4",    
                                  "CHL",       
                                  "DFO-NF",    
                                  "DFO-QCS",   
                                  "EBS",       
                                  "EVHOE",     
                                  "FR-CGFS",   
                                  "GMEX-Summer",
                                  "GOA",       
                                  "GRL-DE",    
                                  "GSL-N",     
                                  "GSL-S",     
                                  "ICE-GFS",   
                                  "IE-IGFS",   
                                  "MEDITS",    
                                  "NAM",       
                                  "NEUS-Fall", 
                                  "NEUS-Spring",
                                  "NIGFS-1",   
                                  "NIGFS-4",   
                                  "Nor-BTS-3", 
                                  "NS-IBTS-1", 
                                  "NS-IBTS-3", 
                                  "NZ-CHAT",   
                                  "NZ-ECSI",   
                                  "NZ-WCSI",   
                                  "PT-IBTS",   
                                  "S-GEORG",   
                                  "SCS-SUMMER",
                                  "SEUS-fall", 
                                  "SEUS-spring",
                                  "SEUS-summer",
                                  "WCANN",     
                                  "ZAF-ATL",   
                                  "ZAF-IND",
                                  "ROCKALL",
                                  "SWC-IBTS-1",
                                  "SWC-IBTS-4",
                                  "FALK",
                                  "GMEX-Fall",
                                  "NZ-SUBA",
                                  "SCS-SPRING"
                          ))

```


####Edit function to create grid which will allow us to maintain similar spatial footprint over time
From [here](https://strimas.com/post/hexagonal-grids/)

We will use cell size of 7,774.2 km^2, as that will match grid cell size of 8 in dggridr. We can't use the package dggridr unfortunately because it doesn't work for this version of R (and others have had this issue too). https://github.com/r-barnes/dggridR
For Norway we will use cell size of 23,322.2 km^2 because the sites are further away from each other.

Make sampling locations into spatial points
```{r sampling locations to sp}
#delete if NA for longitude or latitude
FishGlob.10year.spp_manualclean <- FishGlob.10year.spp_manualclean[complete.cases(FishGlob.10year.spp_manualclean[,.(longitude, latitude)])] 

```

Match lat/lon sampling points to hexagonal cells, so that we can see how many cells to keep to maintain a high portion of observation points

```{r match lat lon to hex cells}
FishGlob.cells <- data.table()

#two potential cell sizes
  #set cell area (depends on whether or not it's Norway)
  cell_area_all  <- 7774.2 #km2 (8 from dggrdr)
  cell_area_norway <- 23322.2 #km2 (7 from dggrdr; if you want to use different resolution, not doing as of now)
  
all_survey_units <- unique(FishGlob.10year.spp_manualclean[,survey_unit])

#baseplot list
fishglob_cell_plots <- list()

#save cells as list of spatial polygons
fishglob_cell_polygons <- list()


for(i in 1:length(all_survey_units)){
  FishGlob.10year.spp_manualclean_subset <- FishGlob.10year.spp_manualclean[survey_unit == all_survey_units[i],]
  
  #unique lat lon combos
  FishGlob.10year.spp_manualclean_subset_unique <- unique(FishGlob.10year.spp_manualclean_subset[,.(longitude,latitude,haul_id,year)])
  
  #coordinates to Spatial Points Object
  if(max(FishGlob.10year.spp_manualclean_subset_unique[,longitude]) - min(FishGlob.10year.spp_manualclean_subset_unique[,longitude]) > 359){ #if survey region crosses dateline, use st_shift_longitude()
    sp <- FishGlob.10year.spp_manualclean_subset_unique %>%
          st_as_sf(coords = c("longitude","latitude"), crs = 4326) %>%
             st_shift_longitude()
  }else{
    sp <- FishGlob.10year.spp_manualclean_subset_unique %>%
          st_as_sf(coords = c("longitude","latitude"), crs = 4326)
  }
  
  sp.t <- as(sp, "Spatial")
  
  proj4string(sp.t) <- CRS("+proj=longlat")
  
proj <-  ifelse(max(FishGlob.10year.spp_manualclean_subset_unique[,longitude]) - min(FishGlob.10year.spp_manualclean_subset_unique[,longitude]) > 359, #if survey region crosses dateline, use +lon_0=-140 instead of +lon_0=0
         "+proj=robin +lon_0=-140 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs",
         "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs")
  
  
  sp.p <- spTransform(sp.t, CRS(proj)) #note km^2 units
  
  #use Concaveman to convert points to polygon
  polygon  <- concaveman(sp, 2, 3)

  polygon_spapol <- as(polygon, "Spatial") #convert simple polygon feature to spatial polygon
  
  proj4string(polygon_spapol) <- CRS("+proj=longlat")
  
  polygon_spapol.p <- spTransform(polygon_spapol, CRS(proj)) #note km^2 units
  
  #create grid 
  #set cell_area based on whether or not it's the Norway survey
  cell_area_km <- ifelse(all_survey_units[i] %in% c("Nor-BTS-1","Nor-BTS-3"), cell_area_norway, cell_area_all) #note this is in kilometers (if you want to use different cell resolution for Norway)
  
  #calculate cell_diameter of hexagons from cell_areas
  cell_diameter_km <- sqrt(2 * cell_area_km / sqrt(3)) # in meters
  

  ext <- as(extent(polygon_spapol.p)
            + 2*cell_diameter_km #add a buffer to make sure all observations are assigned a cell
             , "SpatialPolygons")
 # plot(ext)
 # plot(sp.p, add = T, pch = ".")
  
  projection(ext) <- projection(polygon_spapol.p) #match projection
  
  # generate array of hexagon centers
  g <- spsample(ext, type = "hexagonal", cellsize = cell_diameter_km, offset = c(0.5, 0.5))
  
    # convert center points to hexagons
  g <- HexPoints2SpatialPolygons(g, dx = cell_diameter_km)
  
  #save spatial polygons
  fishglob_cell_polygons[[i]] <- g
  
  #convert hexagons to simple features
  g_sf <- sf::st_as_sf(g, coords = c("x","y"))
  
  g_sf.t <- st_make_valid(g_sf)
  
 fishglob_cell_plots[[i]] <- ggplot() +
   geom_sf(data = sp, size = 0.2, color = "darkgrey") +
   geom_sf(data = g_sf.t, fill = NA, color = "black") +
   theme_classic() +
   ggtitle(paste0(all_survey_units[i]))
   
   
  #base R plot, replaced by fishglob_cell_plots[[i]]
#  plot(g)
# plot(sp.p, add = T, pch = ".")
# title(paste0(all_survey_units[i]))
 
 #

  #link lat lon to cell#
    #where do they overlap
    sp.p$cell_ID <- over(sp.p,g) #over(x=location of queries, y = layer from which geometries are queried)
    
    #link lat long to cell #s
    FishGlob.10year.spp_manualclean_subset_unique[,cell_ID := sp.p$cell_ID][,cell_year_count := .N, .(cell_ID, year)]
    
    #link back to subsetted database of observations
    FishGlob.subset.cells <- FishGlob.10year.spp_manualclean_subset[FishGlob.10year.spp_manualclean_subset_unique, on = c("longitude", "latitude","year","haul_id")]
    
    FishGlob.cells <- rbind(FishGlob.cells, FishGlob.subset.cells)

    
#make sure all projections match for binding of polygons
polygon_spapol.forbind <- spTransform(polygon_spapol.p,
                                      CRS=CRS( "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs"))

      polygon_spapol.forbind$survey_unit <- all_survey_units[i]
      
    #bind polygons into spdf
    if(i ==1){
      all_survey_units_polygon <- polygon_spapol.forbind #if first, just polygon to start
    }else{
      all_survey_units_polygon <- rbind(all_survey_units_polygon, polygon_spapol.forbind)  #if not first, bind new polygon to first polygon
    }
    
       
  
}
  
```

________

###Standardize observations by # of tows per cell and # cells sampled per year

Remove any years for a survey that sample less than 70% of cells ever sampled for that survey
THEN remove any cells that are sampled in less than 70% of years

```{r standardize observations all regions}

#account of data loss by standardization
data_loss <- data.table(survey_unit=all_survey_units,
                        year_threshold=as.numeric(NA),
                        cell_threshold=as.numeric(NA),
                        percent_years_excluded=as.numeric(NA),
                        percent_tows_excluded_by_year=as.numeric(NA),
                        percent_obs_excluded_by_year=as.numeric(NA),
                        percent_cells_excluded=as.numeric(NA),
                        percent_tows_excluded_total=as.numeric(NA),
                        percent_obs_excluded_total=as.numeric(NA))

FishGlob.wellsampledyearscells_complete <- data.table()

#empty vector for plots with years excluded
fishglob_exclude_years_plots <- list()
#empty vector for plots with cells excluded
fishglob_exclude_cells_plots <- list()

for (i in 1:length(all_survey_units)) {
      
      #subset to region
      FishGlob.cells.subset <- FishGlob.cells[survey_unit == all_survey_units[i],]
      
      #new column calculate tows per year
      FishGlob.cells.subset[,tows_per_year := uniqueN(haul_id),.(year,survey_unit)]
      
      #unique year, #cells  sampled
      year_cells_sampled <- unique(FishGlob.cells.subset[,.(year,cell_ID)])
      year_cells_sampled <- year_cells_sampled[,yearly_cell_count := .N,year]
      year_cells_sampled <- unique(year_cells_sampled[,.(year,yearly_cell_count)])
      
      #eliminate years with less than 70% of cells sampled
      year_benchmark <- 0.70
      benchmark_value <- year_benchmark*max(year_cells_sampled[,yearly_cell_count])
      
      #only keep years where over 70% of cells are sampled
      year_cells_sampled[,benchmark := yearly_cell_count >= benchmark_value]
      
      years_deleted <- unique(year_cells_sampled[benchmark == F,year]) #which years are left out?
      
      years_kept <-unique(year_cells_sampled[benchmark ==T,year]) #which years to keep
      
      years_deleted_percent <- round(length(years_deleted)/length(unique(year_cells_sampled[,year]))*100,1)
      

       #print the years that are left out
      print(ifelse(length(years_deleted) == 0, paste0(all_survey_units[i], " Years left out = 0"), paste0(" ",all_survey_units[i], " Years left out = ", years_deleted, collapse = ",")))
     
     print(paste0(years_deleted_percent, "% of Years Excluded"))
      
     
     #mark years excluded
     FishGlob.cells.subset[,`Excluded year` := ifelse(year %in% years_kept, "No","Yes")]
     
     
      #reduce to years that are well sampled
      FishGlob.cells.subset.wellsampledyears <- FishGlob.cells.subset[year %in% years_kept,]
      
     
      #how many observations does this remove?
      percent_obs_removed_year <- round((nrow(FishGlob.cells.subset)-nrow(FishGlob.cells.subset.wellsampledyears))/nrow(FishGlob.cells.subset)*100,2)
      
      percent_tows_removed_year <- round((length(unique(FishGlob.cells.subset[,haul_id]))-length(unique(FishGlob.cells.subset.wellsampledyears[,haul_id])))/length(unique(FishGlob.cells.subset[,haul_id]))*100,2)
      
    #plot years that are removed
     fishglob_exclude_years_plots[[i]] <- 
       ggplot() +
  geom_col(data = unique(FishGlob.cells.subset[,.(year, survey_unit, `Excluded year`,tows_per_year)]), aes(x = year, y = tows_per_year, fill = `Excluded year`)) +
       labs(x = "Year",y = "Unique tows per year") +
       scale_fill_manual(values = c("grey","turquoise")) +
  theme_classic() +
       ggtitle(paste0(all_survey_units[i], ", ",percent_tows_removed_year, "% tows excluded" ))
      
      #identify any cells that are not sampled in 70% of years
      FishGlob.cells.subset.wellsampledyears[,year_cell_count := length(unique(haul_id)),.(year,cell_ID)] # unique haul ids per cell per year
      
      cell_by_year <- unique(FishGlob.cells.subset.wellsampledyears[, .(cell_ID,year)])
      
      cell_by_year[,years_per_cell := .N,cell_ID]
      
      
      #cell ids to remove and keep
      #in any year, which cells are sampled in less than 70% of years
      #we'll make benchmark 70% just for now
      cell_benchmark <- 0.70
      benchmark_value_year_count <- cell_benchmark*max(cell_by_year[,years_per_cell])
      
      cell_id_remove <- unique(cell_by_year[years_per_cell<benchmark_value_year_count,cell_ID])

      
      cells_deleted_percent <- round(length(cell_id_remove)/length(unique(FishGlob.cells.subset.wellsampledyears[,cell_ID]))*100,1)
      
      
      #reduce to cells that are well sampled
      FishGlob.cells.subset.wellsampledyearscells <- FishGlob.cells.subset.wellsampledyears[!(cell_ID %in% cell_id_remove),]
      
      #What percent of tows does this remove?
      tows_removed_yearcell <- round((length(unique(FishGlob.cells.subset[,haul_id]))-length(unique(FishGlob.cells.subset.wellsampledyearscells[,haul_id])))/length(unique(FishGlob.cells.subset[,haul_id]))*100,1) 
      
          #plot cells that are removed
      
      #pull in spatial polygons of cells and extract those that we keep
        #save spatial polygons
      cell_remove_polygon <- fishglob_cell_polygons[[i]][c(cell_id_remove)]
  
  #convert hexagons to simple features
  cell_remove_polygon_sf <- sf::st_as_sf(cell_remove_polygon, coords = c("x","y"))
      
      fishglob_exclude_cells_plots[[i]] <- 
        fishglob_cell_plots[[i]] +
        geom_sf(data= cell_remove_polygon_sf, fill = "turquoise",color = "turquoise", alpha = 0.2)  +
       ggtitle(paste0(all_survey_units[i], ", ",tows_removed_yearcell, "% tows excluded" ))
      
      #What percent of observations does this remove?
      obs_removed_yearcell <- round((nrow(FishGlob.cells.subset)-nrow(FishGlob.cells.subset.wellsampledyearscells))/nrow(FishGlob.cells.subset)*100,1)
      
      cell_id_remove.string <- paste(cell_id_remove, collapse = ", ")
      obs_removed.string <- paste(obs_removed_yearcell, collapse = ", ")
      
      #build data table from this reduced output
      FishGlob.wellsampledyearscells_complete <- rbind(FishGlob.wellsampledyearscells_complete, FishGlob.cells.subset.wellsampledyearscells)
      
      #fill out table with statistics of dropped observations
      data_loss[i, "year_threshold"] = year_benchmark
      data_loss[i, "cell_threshold"] = cell_benchmark
      data_loss[i, "percent_years_excluded"] = years_deleted_percent
      data_loss[i, "percent_tows_excluded_by_year"] = percent_tows_removed_year
      data_loss[i, "percent_obs_excluded_by_year"] = percent_obs_removed_year
      data_loss[i, "percent_cells_excluded"] = cells_deleted_percent
      data_loss[i, "percent_tows_excluded_total"] = tows_removed_yearcell
      data_loss[i, "percent_obs_excluded_total"] = obs_removed_yearcell
      
        #print portion of cells that are left out
      print(ifelse(length(cell_id_remove) == 0, paste0(all_survey_units[i], " Cells left out = 0"), paste0(all_survey_units[i], " Cells left out = ", cell_id_remove.string, ", ",cells_deleted_percent, "% Cells Excluded, ",tows_removed_yearcell,"% Tows Removed, ", obs_removed_yearcell, "% Observations Removed")))
      
      }
     
#visual data cleaning summaries
#years
saveRDS(fishglob_exclude_years_plots, here::here("figures","standardization","fishglob_exclude_years_plots.Rds"))

#cells
saveRDS(fishglob_exclude_cells_plots, here::here("figures","standardization","fishglob_exclude_cells_plots.Rds"))

#now, check again to see if any are less than 10 years
FishGlob.wellsampledyearscells_complete[,years_sampled_update := length(unique(year)),.(survey_unit)]
FishGlob.wellsampledyearscells_complete.10year <- FishGlob.wellsampledyearscells_complete[years_sampled_update >= 10,]

#only keep survey units for which fewer than 50% of tows were deleted by 70% thresholds
survey_units_keep <- unique(data_loss[percent_tows_excluded_total <= 50,survey_unit])

FishGlob.wellsampledyearscells_complete.final <- FishGlob.wellsampledyearscells_complete.10year[survey_unit %in% survey_units_keep,]

saveRDS(FishGlob.wellsampledyearscells_complete.final, here::here("data","cleaned","FishGlob.wellsampledyearscells_complete.final.rds"))

#add more helpful names
data_loss <- name_helper[data_loss, on = "survey_unit"]

#more helpful column names
data_loss.final <- data_loss[,.(survey_unit, Survey_Name_Season, year_threshold, percent_years_excluded, percent_tows_excluded_by_year, cell_threshold, percent_cells_excluded, percent_tows_excluded_total)]

colnames(data_loss.final)<-c("SURVEY_UNIT_ORDER","Survey", "Year inclusion threshold\n(% cells sampled in year)", "Percent years excluded", "Percent tows excluded\nfrom year standardization","Cell inclusion threshold\n(% years sampled in cells)", "Percent cells excluded", "Total percent tows excluded")

#data loss
saveRDS(data_loss, here::here("data","cleaned","data_loss.rds"))
fwrite(data_loss.final, here::here("data","cleaned","data_loss.final.csv"))

#change in haul id count
(length(unique(FishGlob.cells$haul_id))-length(unique(FishGlob.wellsampledyearscells_complete.final$haul_id)))/length(unique(FishGlob.cells$haul_id)) *100

length(unique(FishGlob.wellsampledyearscells_complete.final$haul_id))


#change in observations
(nrow(FishGlob.cells)-nrow(FishGlob.wellsampledyearscells_complete.final))/nrow(FishGlob.cells) *100

#number of unique speciees
length(unique(FishGlob.wellsampledyearscells_complete.final$accepted_name))

```

Are any survey_units missing CPUE?
```{r missing cpue}

View(FishGlob.wellsampledyearscells_complete.final[, .(sum(is.na(wgt_cpue)),.N,(round(sum(is.na(wgt_cpue))/.N*100,1))), survey_unit])
```

Survey Season: NAs for wgt_cpue
MEDITS: 350911 (no weights,  only abundances,  use abundances (num_cpue) instead

Otherwise, very small proportions, so will just exclude these observations.


Delete these observations above except for MEDITS (we will do trends in abundance metrics for this region)

Final cleaning edits
 - Delete all observations without CPUE except for MEDITS
 - Sum duplicates, summing from more than one obs for accepted name
```{r final cleaning edits}
#delete all rows where wgt_cpue is NA,  unless it is MEDITS region
FishGlob_clean.noNA <- FishGlob.wellsampledyearscells_complete.final[(survey != "MEDITS" & !is.na(wgt_cpue)) | survey == "MEDITS"]

#Some spp are duplicated within a tow. In all cases,  this is due to verbatim names matching to a single accepted name. Therefore,  we will sum these wgts and abundances
FishGlob_clean.noNA[, wgt := sum(wgt,  na.rm = T), .(accepted_name,  haul_id)][, 
                     wgt_h := sum(wgt_h,  na.rm = T), .(accepted_name,  haul_id)][, 
                     wgt_cpue := sum(wgt_cpue,  na.rm = T), .(accepted_name,  haul_id)][, 
                     num := sum(num,  na.rm = T), .(accepted_name,  haul_id)][, 
                     num_h := sum(num_h,  na.rm = T), .(accepted_name,  haul_id)][, 
                     num_cpue := sum(num_cpue,  na.rm = T), .(accepted_name,  haul_id)]

FishGlob_clean.noNA <- unique(FishGlob_clean.noNA[,.(survey, haul_id, country, sub_area, continent, stat_rec, station, stratum, year, month, day, quarter, season, latitude, longitude, haul_dur, area_swept, gear, depth, sbt, sst, num, num_h, num_cpue, wgt, wgt_h, wgt_cpue, verbatim_aphia_id, accepted_name, aphia_id, SpecCode, kingdom, phylum, class, order, family, genus, rank, survey_unit, years_sampled, cell_ID, cell_year_count, year_cell_count, years_sampled_update)])

#check
summary(FishGlob_clean.noNA[, wgt_cpue]) #why inf? 13 observations from GSL-N,  I will delete these
FishGlob_clean <- copy(FishGlob_clean.noNA)[!is.infinite(wgt_cpue), ]

saveRDS(FishGlob_clean, here::here("data","cleaned","FishGlob_clean.rds"))


```

