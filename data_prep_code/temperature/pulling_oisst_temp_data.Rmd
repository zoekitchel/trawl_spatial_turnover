---
title: "Pulling OISST Temp Data"
output: html_notebook
---

This code is Script 5 for Kitchel et al. TITLE manuscript.

- This project is a collaborative effort to describe changes in taxonomic composition  of fish communities around the world--as sampled by bottom trawl surveys.

- Code by Zoë J. Kitchel

SESSION INFO TO DO


Daily SST Temperature

Currently not sure why

```{r setup}
library(dplyr) # A staple for modern data management in R
library(lubridate) # Useful functions for dealing with dates
library(ggplot2) # The preferred library for data visualisation
library(tidync) # For easily dealing with NetCDF data
library(rerddap) # For easily downloading subsets of data
library(stringr)
library(data.table)

#pull in data for year and lat lon restrictions
FishGlob_clean <- readRDS(here::here("data", "cleaned", "FishGlob.wellsampledyearscells_complete.final.rds"))

#names of all survey units
all_survey_units <- unique(FishGlob_clean[,.(survey_unit)])
#for annotate (HPC)
#FishGlob_clean <- readRDS(here::here("OISST_temp_data_homogenization","FishGlob.wellsampledyearscells_complete.final.rds"))

```

Unique Lat Lon Year for each Survey/season
```{r unique lat lon}
unique_lat_lon_year <- unique(FishGlob_clean[,.(latitude, longitude, year, survey, survey_unit)])

#data range for this data is (-179.875, 179.875), so I will trim to this
unique_lat_lon_year[,longitude_trim := ifelse(longitude < -179.875, -179.875, ifelse(longitude > 179.875, 179.875, longitude))]

summary(unique_lat_lon_year[,longitude_trim])
```

Visualize what data look like
```{r viz data}
# The information for the NOAA OISST data
rerddap::info(datasetid = "ncdcOisst21Agg_LonPM180", url = "https://coastwatch.pfeg.noaa.gov/erddap/")

# Note that there is also a version with lon values from 0 yo 360
rerddap::info(datasetid = "ncdcOisst21Agg", url = "https://coastwatch.pfeg.noaa.gov/erddap/")
```

Write function to prep data based on start and end dates and provided lat long
```{r function to prep data}
# This function downloads and prepares data based on user provided start and end dates
OISST_sub_dl <- function(time_df, this_survey_unit){
  OISST_dat <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                       url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                       time = c(time_df$start, time_df$end), 
                       zlev = c(0, 0),
                       latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                    max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                       longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                    max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                       fields = "sst")$data %>% 
    mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
    dplyr::rename(t = time, temp = sst) %>% 
    select(lon, lat, t, temp) %>% 
    na.omit()
}
```

Setup dates we will pull data for. For ease, I'm going to pull entire time period
```{r setup dates in dl_year}
    
dl_years <- data.frame(date_index = 1:5,
                           start = as.Date(c("1981-09-01", "1989-01-01", 
                                             "1997-01-01", "2005-01-01", "2013-01-01")),
                           end = as.Date(c("1988-12-31", "1996-12-31", 
                                           "2004-12-31", "2012-12-31", "2021-12-31")))



```

I am running AI separately because it spans dateline, and I need to do some edits

```{r AI alone}
# Download all of the data with one nested request
# The time this takes will vary greatly based on connection speed

    system.time(
      OISST_data <- dl_years %>% 
        group_by(date_index) %>% 
        group_modify(~OISST_sub_dl(.x, "AI")) %>% 
        ungroup() %>% 
        select(lon, lat, t, temp)
    ) # 38 seconds, ~8 seconds per batch


OISST_data_AI <- as.data.table(OISST_data)

AI_min_positive <- min(unique_lat_lon_year[longitude > 150 & survey == "AI"]$longitude)
AI_max_negative <-max(unique_lat_lon_year[longitude < 0 & survey == "AI"]$longitude)

OISST_data_AI <- OISST_data_AI[lon >= AI_min_positive | lon <= AI_max_negative,][,lon_s := ifelse(lon > 160, lon-360, lon)]

#edit, because we don't actually need all of this data
hist(FishGlob_clean[survey == "AI", longitude])
#for just AI 
#2024.317  159.159 4585.945 

#save AI data
saveRDS(OISST_data_AI, here::here("data","Temperature","OISST_data_AI.rds"))
```

I am running MEDITS separately because there is an issue with pulling data after 2019. Data ends in 2019, so I will only pull data before that
```{r}
#run MEDITS separately
MEDITS_dat_1 <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                     url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                     time = c("1981-09-01", "1988-12-31"), 
                     zlev = c(0, 0),
                     latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                  max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                     longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                   max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                     fields = "sst")$data %>% 
  mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
  dplyr::rename(t = time, temp = sst) %>% 
  dplyr::select(lon, lat, t, temp) %>% 
  na.omit()

MEDITS_dat_2 <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                        url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                        time = c("1989-01-01", "1996-12-31"), 
                        zlev = c(0, 0),
                        latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                     max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                        longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                      max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                        fields = "sst")$data %>% 
  mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
  dplyr::rename(t = time, temp = sst) %>% 
  dplyr::select(lon, lat, t, temp) %>% 
  na.omit()

MEDITS_dat_3 <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                        url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                        time = c("1997-01-01", "2004-12-31"), 
                        zlev = c(0, 0),
                        latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                     max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                        longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                      max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                        fields = "sst")$data %>% 
  mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
  dplyr::rename(t = time, temp = sst) %>% 
  dplyr::select(lon, lat, t, temp) %>% 
  na.omit()

MEDITS_dat_4 <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                        url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                        time = c("2005-01-01", "2012-12-31"), 
                        zlev = c(0, 0),
                        latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                     max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                        longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                      max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                        fields = "sst")$data %>% 
  mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
  dplyr::rename(t = time, temp = sst) %>% 
  dplyr::select(lon, lat, t, temp) %>% 
  na.omit()

MEDITS_dat_5 <- griddap(x = "ncdcOisst21Agg_LonPM180", 
                        url = "https://coastwatch.pfeg.noaa.gov/erddap/", 
                        time = c("2013-01-01", "2019-12-31"), #note date end edit (2021 to 2019)
                        zlev = c(0, 0),
                        latitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude),
                                     max(unique_lat_lon_year[survey_unit == this_survey_unit]$latitude)),
                        longitude = c(min(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim),
                                      max(unique_lat_lon_year[survey_unit == this_survey_unit]$longitude_trim)),
                        fields = "sst")$data %>% 
  mutate(time = as.Date(stringr::str_remove(time, "T00:00:00Z"))) %>% 
  dplyr::rename(t = time, temp = sst) %>% 
  dplyr::select(lon, lat, t, temp) %>% 
  na.omit()

MEDITS_data <- rbind(MEDITS_dat_1, MEDITS_dat_2, MEDITS_dat_3, MEDITS_dat_4, MEDITS_dat_5)

saveRDS(MEDITS_data,
        here::here("data","Temperature","OISST_data_MEDITS.rds"))

#for annotate (HPC)
saveRDS(MEDITS_data,
        here::here("OISST_data_MEDITS.rds"))
```


Plot temp for one period in AI just to make sure it looks right

```{r test plot AI}
OISST_data_AI %>% 
  filter(t == "2018-12-01") %>% 
  ggplot(aes(x = lon_s, y = lat)) +
  geom_tile(aes(fill = temp)) +
  borders() + # Activate this line to see the global map
  scale_fill_viridis_c() +
  coord_quickmap(expand = F) +
  labs(x = NULL, y = NULL, fill = "SST (°C)") +
  theme(legend.position = "bottom")
```

Loop to pull data for all regions (works best on remote server, takes quite a bit of time)
```{r loop to pull in all regions past AI}
for (i in 2:length(all_survey_units)) { #skip AI, pull separately because it spans dateline
    system.time(
      OISST_data <- dl_years %>% 
        group_by(date_index) %>% 
        group_modify(~OISST_sub_dl(.x, all_survey_units[i])) %>% 
        ungroup() %>% 
        select(lon, lat, t, temp)
    ) # 38 seconds, ~8 seconds per batch

    assign(paste0("OISST_data_",all_survey_units[i]), OISST_data) #check this
    
    saveRDS(get(paste0("OISST_data_",all_survey_units[i])), here::here("data","Temperature",paste0("OISST_data_",all_survey_units[i],".rds")))
    
}
```

Average Values

First go from daily temp to average monthly temp. We need mean temp 12 months before May, max temp 12 months before May, and min temp 12 months before June. At some time in the future, I may want to extract values at exact temp values, but not sure it matters much now. Instead, I'll take yearly:
* mean
* max
* min
* seas
* SD of mean
* SD of max
* SD of min 
* SD of seas

And now all other regions
```{r loop for average values}

#to maximize memory, remove FishGlob_clean except for min_month
FishGlob_clean.months_surveys <- unique(FishGlob_clean[,.(survey_unit, month)])
FishGlob_clean.months_surveys[survey_unit == "MEDITS", month := 4]
FishGlob_clean.months_surveys[survey_unit == "GRL-DE", month := 8] #NEED TO CHECK THIS

#Greenland and Mediterranean don't have months
#Mediterranean: All observations in quarter 2, so I will say April as month 1

rm(FishGlob_clean)

OISST_data_temp_avgs_full <- data.table()

for (i in 1:length(all_survey_units)) {
  min_month <- min(FishGlob_clean.months_surveys[survey_unit == all_survey_units[i]]$month)
  
  #pull in data from rds files if not already in console
  
  OISST_data <- data.table(readRDS(here::here("data","Temperature",paste0("OISST_data_",all_survey_units[i],".rds"))))
  
  #for annotate (HPC)
#  OISST_data <- data.table(readRDS(here::here(paste0("OISST_data_",all_survey_units[i],".rds"))))
  
  OISST_data[,month := month(t)][,year := year(t)]
  
#set year for taking temp measurements (12 months before first haul for that survey unit)

  OISST_data[, year_for_avg := ifelse(month >= min_month, year+1, year)]
  
  #next two steps take a lot of time
  
#take mean, min, max, and seas of each lat long point for each year 
  OISST_data[, yearly_mean_bypoint := mean(temp), .(year_for_avg, lat, lon)][,
              yearly_max_bypoint := max(temp), .(year_for_avg, lat, lon)][,                                                                yearly_min_bypoint := min(temp), .(year_for_avg, lat, lon)][,   
              yearly_seas_bypoint := yearly_max_bypoint-yearly_min_bypoint]
  
  #cuts down size of object from hundreds of millions to 
  OISST_data <- unique(OISST_data[,.(yearly_mean_bypoint, yearly_max_bypoint, yearly_min_bypoint, yearly_seas_bypoint, lat, lon, year_for_avg)])
  
#next, we want yearly avg of mean, min, max, and seas
    OISST_data[, yearly_mean_bypoint_avg := mean(yearly_mean_bypoint), .(year_for_avg)][,
              yearly_max_bypoint_avg := mean(yearly_max_bypoint), .(year_for_avg)][,                                                       yearly_min_bypoint_avg := mean(yearly_min_bypoint), .(year_for_avg)][,   
              yearly_seas_bypoint_avg := mean(yearly_seas_bypoint), .(year_for_avg)]
  
  #next, to calculate SD of mean, take mean of each cell in each year, and then calculate the SD of the means
  #to calculate SD of max, take max of each cell in each year, and then calculate the SD of the maxes, etc.
    OISST_data[, yearly_mean_bypoint_SD := sd(yearly_mean_bypoint), .(year_for_avg)][,
              yearly_max_bypoint_SD := sd(yearly_max_bypoint), .(year_for_avg)][,                                                       yearly_min_bypoint_SD := sd(yearly_min_bypoint), .(year_for_avg)][,   
              yearly_seas_bypoint_SD := sd(yearly_seas_bypoint), .(year_for_avg)]

   
  #finally, scaled versions by region
#next, we want yearly avg of mean, min, max, and seas
    OISST_data[, yearly_mean_bypoint_avg.s := scale(yearly_mean_bypoint_avg)][,
                 yearly_max_bypoint_avg.s := scale(yearly_max_bypoint_avg)][,                                                                        yearly_min_bypoint_avg.s := scale(yearly_min_bypoint_avg)][,   
                 yearly_seas_bypoint_avg.s := scale(yearly_seas_bypoint_avg)]
  
  #next, to calculate SD of mean, take mean of each cell in each year, and then calculate the SD of the means
  #to calculate SD of max, take max of each cell in each year, and then calculate the SD of the maxes, etc.
    OISST_data[, yearly_mean_bypoint_SD.s := scale(yearly_mean_bypoint_SD)][,
              yearly_max_bypoint_SD.s := scale(yearly_max_bypoint_SD)][,                                                                           yearly_min_bypoint_SD.s := scale(yearly_min_bypoint_SD)][,   
              yearly_seas_bypoint_SD.s := scale(yearly_seas_bypoint_SD)]
  
#uniquevalues
    OISST_data_temp_avgs <- unique(OISST_data[, .(year_for_avg, yearly_mean_bypoint_avg, yearly_max_bypoint_avg, yearly_min_bypoint_avg, yearly_seas_bypoint_avg, yearly_mean_bypoint_SD, yearly_max_bypoint_SD, yearly_min_bypoint_SD, yearly_seas_bypoint_SD, yearly_mean_bypoint_avg.s, yearly_max_bypoint_avg.s, yearly_min_bypoint_avg.s, yearly_seas_bypoint_avg.s, yearly_mean_bypoint_SD.s, yearly_max_bypoint_SD.s, yearly_min_bypoint_SD.s, yearly_seas_bypoint_SD.s)])
  
  OISST_data_temp_avgs[, survey_unit := all_survey_units[i]]
  
  OISST_data_temp_avgs_full <- rbind(OISST_data_temp_avgs_full, OISST_data_temp_avgs)

  
  print(all_survey_units[[i]]) #to help keep track


}


#save summary table!!!
saveRDS(OISST_data_temp_avgs_full, here::here("data","Temperature","OISST_data_temp_avgs_full.rds"))

#annotate
#saveRDS(OISST_data_temp_avgs_full, here::here("OISST_data_temp_avgs_full.rds"))

```


